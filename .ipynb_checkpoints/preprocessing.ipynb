{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4e304b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  Unnamed: 1\n",
       "0                            Bon retour         NaN\n",
       "1                            Insistance         NaN\n",
       "2                            Insistance         NaN\n",
       "3                                Rappel         NaN\n",
       "4      Elle a un bon retour sur produit         NaN\n",
       "...                                 ...         ...\n",
       "69892                        Insistance         NaN\n",
       "69893                        Insistance         NaN\n",
       "69894                        Insistance         NaN\n",
       "69895                        Insistance         NaN\n",
       "69896                        Insistance         NaN\n",
       "\n",
       "[69897 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "#Les bibliotheques a instalée\n",
    "\n",
    "#pip install autocorrect\n",
    "#pip install -U spacy\n",
    "#python -m spacy download fr_core_news_sm\n",
    "#pip install spellchecker\n",
    "\n",
    "\n",
    "# Chemins des fichiers XLSX\n",
    "file1 = 'resources/common/data/all_raw_comments_cleaning.xlsx'\n",
    "\n",
    "# Lecture des fichiers XLSX\n",
    "dfnew_comment = pd.read_excel(file1)\n",
    "dfnew_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5cf2c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69897 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment\n",
       "0                            Bon retour\n",
       "1                            Insistance\n",
       "2                            Insistance\n",
       "3                                Rappel\n",
       "4      Elle a un bon retour sur produit\n",
       "...                                 ...\n",
       "69892                        Insistance\n",
       "69893                        Insistance\n",
       "69894                        Insistance\n",
       "69895                        Insistance\n",
       "69896                        Insistance\n",
       "\n",
       "[69897 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew_comment.drop('Unnamed: 1', axis=1, inplace=True)\n",
    "dfnew_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fabef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "361ded62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # check if text is a string\n",
    "    if isinstance(text, str):\n",
    "        # remove old style retweet text \"RT\"\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "\n",
    "        # remove hyperlinks\n",
    "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "\n",
    "        # remove hashtags (only removing the hash # sign from the word)\n",
    "        text = re.sub(r'#', '', text)\n",
    "\n",
    "        # remove dates in format YYYY-MM-DD\n",
    "        text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '', text)\n",
    "\n",
    "        # remove time in format HH:MM or HH:MM:SS\n",
    "        text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text)\n",
    "\n",
    "        # remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "# remove empty lines or lines with just a dot\n",
    "        text = re.sub(r'^(\\s*\\.?\\s*)$', '', text, flags=re.MULTILINE)\n",
    "    else:\n",
    "        # if text is a number, convert it to a string\n",
    "        if isinstance(text, (int, float)):\n",
    "            text = str(text)\n",
    "        # if text is a NaN value, replace it with an empty string\n",
    "        elif pd.isnull(text):\n",
    "            text = ''\n",
    "\n",
    "    return text.strip()  # strip leading/trailing white spaces\n",
    "\n",
    "# apply the function to each element of the DataFrame\n",
    "dfnew_comment = dfnew_comment.applymap(clean_text)\n",
    "\n",
    "# replace lines that are just a dot or a comma (now an empty string after removing special characters) with NaN\n",
    "dfnew_comment.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# remove lines with NaN values\n",
    "dfnew_comment.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "print( dfnew_comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "721c5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df=dfnew_comment\n",
    "print(df)\n",
    "print(type(df))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5f8c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     comment\n",
      "0                            \u001b[92mBon retour\n",
      "1                            \u001b[92mInsistance\n",
      "2                            \u001b[92mInsistance\n",
      "3                                \u001b[92mRappel\n",
      "4      \u001b[92mElle a un bon retour sur produit\n",
      "...                                      ...\n",
      "69892                        \u001b[92mInsistance\n",
      "69893                        \u001b[92mInsistance\n",
      "69894                        \u001b[92mInsistance\n",
      "69895                        \u001b[92mInsistance\n",
      "69896                        \u001b[92mInsistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "\u001b[94m\n",
      "\n",
      "DataFrame après tokenisation:\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print('\\033[92m' + df)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# Tokeniser les textes dans la colonne 'comment'\n",
    "df['tokens'] = df['comment'].apply(tokenizer.tokenize)\n",
    "\n",
    "# Afficher le DataFrame après tokenisation\n",
    "print(\"\\nDataFrame après tokenisation:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9001c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "428408f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tokens_no_numbers = [token for token in df_tokens if not token.isdigit()]\n",
    "\n",
    "#df_tokens=df_tokens_no_numbers\n",
    "#print(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "450dc98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bedhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# You need to download the stopwords package from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_french = stopwords.words('french')\n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_french)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95a2ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \n",
      "0                                 [bon, retour]              [bon, retour]  \n",
      "1                                  [insistance]               [insistance]  \n",
      "2                                  [insistance]               [insistance]  \n",
      "3                                      [rappel]                   [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]  \n",
      "...                                         ...                        ...  \n",
      "69892                              [insistance]               [insistance]  \n",
      "69893                              [insistance]               [insistance]  \n",
      "69894                              [insistance]               [insistance]  \n",
      "69895                              [insistance]               [insistance]  \n",
      "69896                              [insistance]               [insistance]  \n",
      "\n",
      "[69885 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\033[94m')\n",
    "\n",
    "df_clean = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df['tokens']:\n",
    "    clean_tokens = []  # Liste pour stocker les tokens nettoyés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Vérifier si le mot n'est pas un mot d'arrêt et n'est pas un signe de ponctuation\n",
    "        if word not in stopwords_french and word not in string.punctuation:\n",
    "            clean_tokens.append(word)\n",
    "    # Ajouter les tokens nettoyés de ce texte à la liste df_clean\n",
    "    df_clean.append(clean_tokens)\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_clean comme une nouvelle colonne à votre DataFrame\n",
    "df['clean_tokens'] = df_clean\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f37e8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "                   lemmatized_tokens  \n",
      "0                      [bon, retour]  \n",
      "1                       [insistance]  \n",
      "2                       [insistance]  \n",
      "3                           [rappel]  \n",
      "4      [avoir, bon, retour, produit]  \n",
      "...                              ...  \n",
      "69892                   [insistance]  \n",
      "69893                   [insistance]  \n",
      "69894                   [insistance]  \n",
      "69895                   [insistance]  \n",
      "69896                   [insistance]  \n",
      "\n",
      "[69885 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the French language model from spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Create a list to store the lemmatized tokens\n",
    "df_lemmatized = []\n",
    "\n",
    "# Iterate over each list of tokens\n",
    "for tokens in df_clean:\n",
    "    lemmatized_tokens = []  # List to store the lemmatized tokens of a particular text\n",
    "    for word in tokens:  # Iterate over each word in the token list\n",
    "        # Lemmatize the word\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else word\n",
    "        lemmatized_tokens.append(lemma)  # Add to the list\n",
    "    # Add the lemmatized tokens of this text to the df_lemmatized list\n",
    "    df_lemmatized.append(lemmatized_tokens)\n",
    "\n",
    "# You can now add df_lemmatized as a new column to your DataFrame\n",
    "df['lemmatized_tokens'] = df_lemmatized\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af806f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  \\\n",
       "0                            Bon retour   \n",
       "1                            Insistance   \n",
       "2                            Insistance   \n",
       "3                                Rappel   \n",
       "4      Elle a un bon retour sur produit   \n",
       "...                                 ...   \n",
       "69892                        Insistance   \n",
       "69893                        Insistance   \n",
       "69894                        Insistance   \n",
       "69895                        Insistance   \n",
       "69896                        Insistance   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "                   lemmatized_tokens  \n",
       "0                      [bon, retour]  \n",
       "1                       [insistance]  \n",
       "2                       [insistance]  \n",
       "3                           [rappel]  \n",
       "4      [avoir, bon, retour, produit]  \n",
       "...                              ...  \n",
       "69892                   [insistance]  \n",
       "69893                   [insistance]  \n",
       "69894                   [insistance]  \n",
       "69895                   [insistance]  \n",
       "69896                   [insistance]  \n",
       "\n",
       "[69885 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66758f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  \\\n",
       "0                            Bon retour   \n",
       "1                            Insistance   \n",
       "2                            Insistance   \n",
       "3                                Rappel   \n",
       "4      Elle a un bon retour sur produit   \n",
       "...                                 ...   \n",
       "69892                        Insistance   \n",
       "69893                        Insistance   \n",
       "69894                        Insistance   \n",
       "69895                        Insistance   \n",
       "69896                        Insistance   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "                   lemmatized_tokens  \n",
       "0                      [bon, retour]  \n",
       "1                       [insistance]  \n",
       "2                       [insistance]  \n",
       "3                           [rappel]  \n",
       "4      [avoir, bon, retour, produit]  \n",
       "...                              ...  \n",
       "69892                   [insistance]  \n",
       "69893                   [insistance]  \n",
       "69894                   [insistance]  \n",
       "69895                   [insistance]  \n",
       "69896                   [insistance]  \n",
       "\n",
       "[69885 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the French language model from spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Create a WordNetLemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list to store the lemmatized tokens\n",
    "df_lemmatized = []\n",
    "\n",
    "# Iterate over each list of tokens\n",
    "for tokens in df_clean:\n",
    "    lemmatized_tokens = []  # List to store the lemmatized tokens of a particular text\n",
    "    for word in tokens:  # Iterate over each word in the token list\n",
    "        # Lemmatize the word using spaCy and fallback to WordNetLemmatizer\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemma)  # Add to the list\n",
    "    # Add the lemmatized tokens of this text to the df_lemmatized list\n",
    "    df_lemmatized.append(lemmatized_tokens)\n",
    "\n",
    "# You can now add df_lemmatized as a new column to your DataFrame\n",
    "df['lemmatized_tokens'] = df_lemmatized\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c304e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  \\\n",
       "0                            Bon retour   \n",
       "1                            Insistance   \n",
       "2                            Insistance   \n",
       "3                                Rappel   \n",
       "4      Elle a un bon retour sur produit   \n",
       "...                                 ...   \n",
       "69892                        Insistance   \n",
       "69893                        Insistance   \n",
       "69894                        Insistance   \n",
       "69895                        Insistance   \n",
       "69896                        Insistance   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "                   lemmatized_tokens             stemmed_tokens  \n",
       "0                      [bon, retour]              [bon, retour]  \n",
       "1                       [insistance]                   [insist]  \n",
       "2                       [insistance]                   [insist]  \n",
       "3                           [rappel]                   [rappel]  \n",
       "4      [avoir, bon, retour, produit]  [a, bon, retour, produit]  \n",
       "...                              ...                        ...  \n",
       "69892                   [insistance]                   [insist]  \n",
       "69893                   [insistance]                   [insist]  \n",
       "69894                   [insistance]                   [insist]  \n",
       "69895                   [insistance]                   [insist]  \n",
       "69896                   [insistance]                   [insist]  \n",
       "\n",
       "[69885 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "# Create a French stemmer instance\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "# Create a list to store the stemmed tokens\n",
    "df_stemmed = []\n",
    "\n",
    "# Iterate over each list of tokens\n",
    "for tokens in df_clean:\n",
    "    stemmed_tokens = []  # List to store the stemmed tokens of a particular text\n",
    "    for word in tokens:  # Iterate over each word in the token list\n",
    "        # Stem the word using the French stemmer\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_tokens.append(stemmed_word)  # Add to the list\n",
    "    # Add the stemmed tokens of this text to the df_stemmed list\n",
    "    df_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# You can now add df_stemmed as a new column to your DataFrame\n",
    "df['stemmed_tokens'] = df_stemmed\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c0c8c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:609\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(path, encoding, comment)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[39myield\u001b[39;00m line\n\u001b[1;32m--> 609\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m lemmatized_tokens \u001b[39m=\u001b[39m []  \u001b[39m# List to store the lemmatized tokens of a particular text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens:  \u001b[39m# Iterate over each word in the token list\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Lemmatize the word using the Pattern library\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     parsed_word \u001b[39m=\u001b[39m parse(word, lemmata\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     12\u001b[0m     lemma \u001b[39m=\u001b[39m parsed_word[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlemma \u001b[39mif\u001b[39;00m parsed_word \u001b[39melse\u001b[39;00m word\n\u001b[0;32m     13\u001b[0m     lemmatized_tokens\u001b[39m.\u001b[39mappend(lemma)  \u001b[39m# Add to the list\u001b[39;00m\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\fr\\__init__.py:219\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(s, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(s, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    217\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns a tagged Unicode string.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mparse(s, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:1172\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self, s, tokenize, tags, chunks, relations, lemmata, encoding, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[39m# Tagger (required by chunker, labeler & lemmatizer).\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mor\u001b[39;00m chunks \u001b[39mor\u001b[39;00m relations \u001b[39mor\u001b[39;00m lemmata:\n\u001b[1;32m-> 1172\u001b[0m     s[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_tags(s[i], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1174\u001b[0m     s[i] \u001b[39m=\u001b[39m [[w] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m s[i]]\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\fr\\__init__.py:163\u001b[0m, in \u001b[0;36mParser.find_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtagset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m UNIVERSAL:\n\u001b[0;32m    162\u001b[0m     kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlambda\u001b[39;00m token, tag: penntreebank2universal(token, tag))\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m _Parser\u001b[39m.\u001b[39mfind_tags(\u001b[39mself\u001b[39m, tokens, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:1112\u001b[0m, in \u001b[0;36mParser.find_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Annotates the given list of tokens with part-of-speech tags.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[39m    Returns a list of tokens, where each token is now a [word, tag]-list.\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[39m# [\"The\", \"cat\", \"purs\"] => [[\"The\", \"DT\"], [\"cat\", \"NN\"], [\"purs\", \"VB\"]]\u001b[39;00m\n\u001b[1;32m-> 1112\u001b[0m \u001b[39mreturn\u001b[39;00m find_tags(tokens,\n\u001b[0;32m   1113\u001b[0m             lexicon \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mlexicon\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlexicon \u001b[39mor\u001b[39;49;00m {}),\n\u001b[0;32m   1114\u001b[0m               model \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel),\n\u001b[0;32m   1115\u001b[0m          morphology \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmorphology\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmorphology),\n\u001b[0;32m   1116\u001b[0m             context \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext),\n\u001b[0;32m   1117\u001b[0m            entities \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentities\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentities),\n\u001b[0;32m   1118\u001b[0m            language \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanguage),\n\u001b[0;32m   1119\u001b[0m             default \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault),\n\u001b[0;32m   1120\u001b[0m                 \u001b[39mmap\u001b[39;49m \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmap\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:1537\u001b[0m, in \u001b[0;36mfind_tags\u001b[1;34m(tokens, lexicon, model, morphology, context, entities, default, language, map, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[39m# Tag words by context.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     tagged \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mapply(tagged)\n\u001b[0;32m   1538\u001b[0m \u001b[39m# Tag named entities.\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m entities \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:877\u001b[0m, in \u001b[0;36mContext.apply\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    875\u001b[0m t \u001b[39m=\u001b[39m o \u001b[39m+\u001b[39m tokens \u001b[39m+\u001b[39m o\n\u001b[0;32m    876\u001b[0m \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(t):\n\u001b[1;32m--> 877\u001b[0m     \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m    878\u001b[0m         \u001b[39mif\u001b[39;00m token[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSTAART\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    879\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:443\u001b[0m, in \u001b[0;36mlazylist.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy(\u001b[39m\"\u001b[39;49m\u001b[39m__iter__\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:432\u001b[0m, in \u001b[0;36mlazylist._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" If the list is empty, calls lazylist.load().\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m    Replaces lazylist.method() with list.method() and calls it.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    433\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, method, types\u001b[39m.\u001b[39mMethodType(\u001b[39mgetattr\u001b[39m(\u001b[39mlist\u001b[39m, method), \u001b[39mself\u001b[39m))\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mlist\u001b[39m, method)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:868\u001b[0m, in \u001b[0;36mContext.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    867\u001b[0m     \u001b[39m# [\"VBD\", \"VB\", \"PREVTAG\", \"TO\"]\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m     \u001b[39mlist\u001b[39;49m\u001b[39m.\u001b[39;49mextend(\u001b[39mself\u001b[39;49m, (x\u001b[39m.\u001b[39;49msplit() \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m _read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)))\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\pattern\\text\\__init__.py:868\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    867\u001b[0m     \u001b[39m# [\"VBD\", \"VB\", \"PREVTAG\", \"TO\"]\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m     \u001b[39mlist\u001b[39m\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m, (x\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m _read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "from pattern.fr import parse\n",
    "\n",
    "# Create a list to store the lemmatized tokens\n",
    "df_lemmatized = []\n",
    "\n",
    "# Iterate over each list of tokens\n",
    "for tokens in df_clean:\n",
    "    lemmatized_tokens = []  # List to store the lemmatized tokens of a particular text\n",
    "    for word in tokens:  # Iterate over each word in the token list\n",
    "        # Lemmatize the word using the Pattern library\n",
    "        parsed_word = parse(word, lemmata=True)\n",
    "        lemma = parsed_word[0].lemma if parsed_word else word\n",
    "        lemmatized_tokens.append(lemma)  # Add to the list\n",
    "    # Add the lemmatized tokens of this text to the df_lemmatized list\n",
    "    df_lemmatized.append(lemmatized_tokens)\n",
    "\n",
    "# You can now add df_lemmatized as a new column to your DataFrame\n",
    "df['lemmatized_tokens2'] = df_lemmatized\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flat_list = [item for sublist in df['lemmatized_tokens2'] for item in sublist]\n",
    "\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f779a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5bb9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rappel: 25278\n",
      "prescrir: 18623\n",
      "produit: 13883\n",
      "présentation: 6659\n",
      "préscrit: 6570\n",
      "prescription: 5128\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivité: 3853\n",
      "prescrire: 3657\n",
      "bon: 3471\n",
      "retour: 3409\n",
      "étendu: 1933\n",
      "avoir: 1929\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "prescripteur: 1064\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "promesse: 784\n",
      "représentation: 762\n",
      "demande: 743\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 573\n",
      "satisfaire: 523\n",
      "association: 516\n",
      "place: 484\n",
      "traitement: 463\n",
      "patient: 457\n",
      "chéloïde: 439\n",
      "relai: 421\n",
      "mise: 401\n",
      "très: 386\n",
      "satisfait: 385\n",
      "stock: 372\n",
      "surtout: 368\n",
      "tout: 355\n",
      "cas: 340\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "échantillon: 287\n",
      "poste: 265\n",
      "aussi: 249\n",
      "post: 233\n",
      "utilisation: 228\n",
      "prèscrit: 227\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "flat_list = [item for sublist in df_lemmatized for item in sublist]\n",
    "\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f54e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rappel: 25278\n",
      "prescrir: 18623\n",
      "produit: 13883\n",
      "présentation: 6659\n",
      "préscrit: 6570\n",
      "prescription: 5128\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivité: 3853\n",
      "prescrire: 3657\n",
      "bon: 3471\n",
      "retour: 3409\n",
      "étendu: 1933\n",
      "avoir: 1929\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "prescripteur: 1064\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "promesse: 784\n",
      "représentation: 762\n",
      "demande: 743\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 573\n",
      "satisfaire: 523\n",
      "association: 516\n",
      "place: 484\n",
      "traitement: 463\n",
      "patient: 457\n",
      "chéloïde: 439\n",
      "relai: 421\n",
      "mise: 401\n",
      "très: 386\n",
      "satisfait: 385\n",
      "stock: 372\n",
      "surtout: 368\n",
      "tout: 355\n",
      "cas: 340\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "échantillon: 287\n",
      "poste: 265\n",
      "aussi: 249\n",
      "post: 233\n",
      "utilisation: 228\n",
      "prèscrit: 227\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "flat_list = [item for sublist in df_lemmatized for item in sublist]\n",
    "\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "803c67ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rappel: 25298\n",
      "prescrir: 18623\n",
      "produit: 13883\n",
      "prescr: 10459\n",
      "présent: 6842\n",
      "prescript: 5128\n",
      "insist: 4825\n",
      "va: 4584\n",
      "entrain: 4337\n",
      "exclus: 4155\n",
      "bon: 3471\n",
      "retour: 3409\n",
      "étendu: 1952\n",
      "a: 1909\n",
      "surfac: 1591\n",
      "conseil: 1356\n",
      "prescripteur: 1064\n",
      "fois: 1008\n",
      "quelqu: 997\n",
      "demand: 791\n",
      "promess: 785\n",
      "représent: 762\n",
      "mention: 639\n",
      "disponibl: 627\n",
      "plai: 617\n",
      "g: 583\n",
      "satisfait: 544\n",
      "associ: 523\n",
      "post: 502\n",
      "trait: 497\n",
      "plac: 487\n",
      "patient: 457\n",
      "chéloïd: 440\n",
      "rel: 439\n",
      "mis: 406\n",
      "tres: 386\n",
      "stock: 372\n",
      "surtout: 368\n",
      "satisf: 362\n",
      "cas: 340\n",
      "utilis: 337\n",
      "engag: 304\n",
      "bien: 290\n",
      "échantillon: 287\n",
      "efficac: 262\n",
      "auss: 249\n",
      "comm: 227\n",
      "vaginal: 214\n",
      "condidos: 208\n",
      "apres: 200\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "flat_list = [item for sublist in df['stemmed_tokens'] for item in sublist]\n",
    "\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def corriger_mot(mot):\n",
    "    # Requête HTTP pour obtenir la correction du mot\n",
    "    url = f\"https://languagetool.org/api/v2/check?text={mot}&language=fr&enabledOnly=false\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extraction de la correction proposée\n",
    "    suggestions = data[0]['matches'][0]['replacements']\n",
    "    correction = suggestions[0]['value']\n",
    "    \n",
    "    # Remplacement du mot mal écrit par la correction\n",
    "    mot_corrigé = re.sub(mot, correction, mot)\n",
    "    \n",
    "    return mot_corrige\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4342f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = \"prescr\"\n",
    "mot_corrigé = corriger_mot(mot)\n",
    "print(mot_corrige)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4466b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a1a1062",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspellchecker\u001b[39;00m \u001b[39mimport\u001b[39;00m Speller\n\u001b[0;32m      5\u001b[0m \u001b[39m# Instancier la classe de correction orthographique\u001b[39;00m\n\u001b[0;32m      6\u001b[0m spell \u001b[39m=\u001b[39m Speller(lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\spellchecker\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39mspellchecker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Spellchecker,getInstance\n",
      "File \u001b[1;32md:\\StageJuin2023\\comment_classification\\myenv\\lib\\site-packages\\spellchecker\\core.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minexactsearch\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mindexer\u001b[39;00m \u001b[39mimport\u001b[39;00m DictionaryIndex\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m _detect_lang\n\u001b[0;32m     29\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSpellchecker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgetInstance\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indexer'"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spellchecker import Speller\n",
    "\n",
    "\n",
    "# Instancier la classe de correction orthographique\n",
    "spell = Speller(lang='fr')\n",
    "\n",
    "# Charger le modèle de langue français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Créer une liste vide pour stocker les tokens lemmatisés\n",
    "df_lemmatized = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df_clean:\n",
    "    lemmatized_tokens = []  # Liste pour stocker les tokens lemmatisés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        corrected_word = spell(word)  # Correction orthographique du mot\n",
    "        # Lemmatisation du mot corrigé\n",
    "        doc = nlp(corrected_word)\n",
    "        lemma = doc[0].lemma_ if doc else corrected_word\n",
    "        lemmatized_tokens.append(lemma)  # Ajouter à la liste\n",
    "    # Ajouter les tokens lemmatisés de ce texte à la liste df_lemmatized\n",
    "    df_lemmatized.append(lemmatized_tokens)\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_lemmatized comme une nouvelle colonne à votre DataFrame\n",
    "df['lemmatized_tokens'] = df_lemmatized\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ba1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ad7d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bedhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je veux prescrire un médicament.</td>\n",
       "      <td>[je, veux, prescrir, un, médic, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La prescription est prête.</td>\n",
       "      <td>[la, prescript, est, prêt, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il a prescrit le traitement.</td>\n",
       "      <td>[il, a, prescr, le, trait, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>préscrir le produit</td>\n",
       "      <td>[prescr, le, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text                      stemmed_tokens\n",
       "0  Je veux prescrire un médicament.  [je, veux, prescrir, un, médic, .]\n",
       "1        La prescription est prête.       [la, prescript, est, prêt, .]\n",
       "2      Il a prescrit le traitement.       [il, a, prescr, le, trait, .]\n",
       "3               préscrir le produit               [prescr, le, produit]\n",
       "4                            rappel                            [rappel]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####TEST#######\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Créer un DataFrame exemple\n",
    "data = {'text': ['Je veux prescrire un médicament.', 'La prescription est prête.', 'Il a prescrit le traitement.','préscrir le produit','rappel']}\n",
    "dftest = pd.DataFrame(data)\n",
    "\n",
    "# Créer une instance de racinisateur français\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "# Créer une liste pour stocker les tokens racinisés\n",
    "dftest_stemmed = []\n",
    "\n",
    "# Itérer sur chaque texte dans la colonne 'text' du DataFrame\n",
    "for text in dftest['text']:\n",
    "    # Tokeniser le texte\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    # Raciner chaque mot dans la liste de tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # Ajouter les tokens racinisés de ce texte à la liste df_stemmed\n",
    "    dftest_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# Ajouter df_stemmed comme nouvelle colonne au DataFrame\n",
    "dftest['stemmed_tokens'] = dftest_stemmed\n",
    "\n",
    "# Afficher le DataFrame\n",
    "dftest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "697fa1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bedhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je veux prescrire un médicament.</td>\n",
       "      <td>[je, veux, prescrir, un, médic, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La prescription est prête.</td>\n",
       "      <td>[la, prescript, est, prêt, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il a prescrit le traitement.</td>\n",
       "      <td>[il, a, prescr, le, trait, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>préscrir le produit</td>\n",
       "      <td>[prescr, le, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text                      stemmed_tokens\n",
       "0  Je veux prescrire un médicament.  [je, veux, prescrir, un, médic, .]\n",
       "1        La prescription est prête.       [la, prescript, est, prêt, .]\n",
       "2      Il a prescrit le traitement.       [il, a, prescr, le, trait, .]\n",
       "3               préscrir le produit               [prescr, le, produit]\n",
       "4                            rappel                            [rappel]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####TEST#######\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Créer un DataFrame exemple\n",
    "data = {'text': ['Je veux prescrire un médicament.', 'La prescription est prête.', 'Il a prescrit le traitement.','préscrir le produit','rappel']}\n",
    "dftest = pd.DataFrame(data)\n",
    "\n",
    "# Créer une instance de racinisateur français\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "# Créer une liste pour stocker les tokens racinisés\n",
    "dftest_stemmed = []\n",
    "\n",
    "# Itérer sur chaque texte dans la colonne 'text' du DataFrame\n",
    "for text in dftest['text']:\n",
    "    # Tokeniser le texte\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    # Raciner chaque mot dans la liste de tokens\n",
    "    stemmed_tokens = [stemmer.stem(word[:]) for word in tokens]\n",
    "    # Ajouter les tokens racinisés de ce texte à la liste df_stemmed\n",
    "    dftest_stemmed.append(stemmed_tokens)\n",
    "\n",
    "# Ajouter df_stemmed comme nouvelle colonne au DataFrame\n",
    "dftest['stemmed_tokens'] = dftest_stemmed\n",
    "\n",
    "# Afficher le DataFrame\n",
    "dftest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72da572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
