{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b527872",
   "metadata": {},
   "source": [
    "# Preprocessing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac235dfa",
   "metadata": {},
   "source": [
    "\n",
    "# Importing Libraries, Requirements and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "393c7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: autocorrect in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 2)) (3.5.4)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: keras==2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 7)) (3.7)\n",
      "Requirement already satisfied: stopwords in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 9)) (1.7.1)\n",
      "Requirement already satisfied: h2o in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 10)) (3.40.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (0.10.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.10.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (63.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow->-r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.51.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (23.1.21)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.11.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.30.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (15.0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.19.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from nltk->-r ../requirements.txt (line 7)) (2021.11.10)\n",
      "Requirement already satisfied: click in c:\\users\\msi\\anaconda3\\lib\\site-packages (from nltk->-r ../requirements.txt (line 7)) (8.0.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\msi\\anaconda3\\lib\\site-packages (from h2o->-r ../requirements.txt (line 10)) (0.8.10)\n",
      "Requirement already satisfied: future in c:\\users\\msi\\anaconda3\\lib\\site-packages (from h2o->-r ../requirements.txt (line 10)) (0.18.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy->-r ../requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 2)) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->-r ../requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from jinja2->spacy->-r ../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#Installer tous les packages nécaissaires\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "#ou bien\n",
    "\n",
    "# !pip install autocorrect\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !pip install pyspellchecker\n",
    "# !pip install keras==2.11.0\n",
    "# !pip install xgboost\n",
    "# !pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4715e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5c7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemins des fichiers XLSX\n",
    "file1 = '../resources/common/data/all_raw_comments_cleaning.xlsx'\n",
    "file4 = '../resources/common/data/labled_comments.xlsx'\n",
    "\n",
    "# Lecture des fichiers XLSX\n",
    "dfnew_comment = pd.read_excel(file1)\n",
    "df_labled = pd.read_excel(file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b963061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit ...</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacité produit</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai réclamé régler situation</td>\n",
       "      <td>Refus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais reçu échantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>même sil observe antécédent chéloïde ème jour</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest problème essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score\n",
       "0      aussi absorption directement niveau peau dit ...   Réclamation\n",
       "1                    doute plus pour efficacité produit   Réclamation\n",
       "2                          jai réclamé régler situation         Refus\n",
       "3                    jamais reçu échantillon auparavant         Refus\n",
       "4         même sil observe antécédent chéloïde ème jour   Réclamation\n",
       "...                                                 ...           ...\n",
       "2569                                    vient prescrire        Client\n",
       "2570                      vient prescrire moment visite        Client\n",
       "2571                                      visite rappel  Présentation\n",
       "2572        voi cest problème essentiellement gastrique   Réclamation\n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation\n",
       "\n",
       "[2574 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c56959d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69897 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment\n",
       "0                            Bon retour\n",
       "1                            Insistance\n",
       "2                            Insistance\n",
       "3                                Rappel\n",
       "4      Elle a un bon retour sur produit\n",
       "...                                 ...\n",
       "69892                        Insistance\n",
       "69893                        Insistance\n",
       "69894                        Insistance\n",
       "69895                        Insistance\n",
       "69896                        Insistance\n",
       "\n",
       "[69897 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew_comment.drop('Unnamed: 1', axis=1, inplace=True)\n",
    "dfnew_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dace52",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1a6711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Vérifiez si le texte est une chaîne\n",
    "    if isinstance(text, str):\n",
    "        # Supprimer le texte de retweet de style ancien \"RT\"\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "        # Supprimer les hyperliens\n",
    "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "        # Supprimer les hashtags (seulement supprimer le signe de hash # du mot)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        # Supprimer les dates au format AAAA-MM-JJ\n",
    "        text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '', text)\n",
    "        # Supprimer l'heure au format HH:MM ou HH:MM:SS\n",
    "        text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text)\n",
    "        # Supprimer les caractères spéciaux\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Supprimer les lignes vides ou les lignes avec juste un point\n",
    "        text = re.sub(r'^(\\s*\\.?\\s*)$', '', text, flags=re.MULTILINE)\n",
    "        # Supprimer les accents\n",
    "        text = unidecode(text)\n",
    "    else:\n",
    "        # Si le texte est un nombre, convertissez-le en chaîne de caractères\n",
    "        if isinstance(text, (int, float)):\n",
    "            text = str(text)\n",
    "        # Si le texte est une valeur NaN, remplacez-le par une chaîne vide\n",
    "        elif pd.isnull(text):\n",
    "            text = ''\n",
    "    return text.strip()  # Supprimer les espaces blancs en tête ou en queue\n",
    "# Appliquer la fonction à chaque élément du DataFrame\n",
    "dfnew_comment = dfnew_comment.applymap(clean_text)\n",
    "# Remplacer les lignes qui sont juste un point ou une virgule (maintenant une chaîne vide après avoir supprimé les caractères spéciaux) par NaN\n",
    "dfnew_comment.replace(\"\", np.nan, inplace=True)\n",
    "# Supprimer les lignes avec des valeurs NaN\n",
    "dfnew_comment.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_labled['comment'] = df_labled['comment'].apply(clean_text)\n",
    "\n",
    "# Remplacer les lignes qui sont juste un point ou une virgule (maintenant une chaîne vide après avoir supprimé les caractères spéciaux) par NaN\n",
    "df_labled.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Supprimer les lignes avec des valeurs NaN dans la colonne 'comment'\n",
    "df_labled.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "\n",
    "print(dfnew_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521dffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Renommer dfnew_comment par df\n",
    "df=dfnew_comment\n",
    "print(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9839ab",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af1c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     comment\n",
      "0                            \u001b[92mBon retour\n",
      "1                            \u001b[92mInsistance\n",
      "2                            \u001b[92mInsistance\n",
      "3                                \u001b[92mRappel\n",
      "4      \u001b[92mElle a un bon retour sur produit\n",
      "...                                      ...\n",
      "69892                        \u001b[92mInsistance\n",
      "69893                        \u001b[92mInsistance\n",
      "69894                        \u001b[92mInsistance\n",
      "69895                        \u001b[92mInsistance\n",
      "69896                        \u001b[92mInsistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "\u001b[94m\n",
      "\n",
      "DataFrame après tokenisation:\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print('\\033[92m' + df)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# Tokeniser les textes dans la colonne 'comment'\n",
    "df['tokens'] = df['comment'].apply(tokenizer.tokenize)\n",
    "df_labled['tokens'] = df_labled['comment'].apply(tokenizer.tokenize)\n",
    "# Afficher le DataFrame après tokenisation\n",
    "print(\"\\nDataFrame après tokenisation:\")\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d64946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6bb85",
   "metadata": {},
   "source": [
    "# Importing French Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8404624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords_french = stopwords.words('french')\n",
    "print('Stop words\\n')\n",
    "print(stopwords_french)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e7c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \n",
      "0                                 [bon, retour]              [bon, retour]  \n",
      "1                                  [insistance]               [insistance]  \n",
      "2                                  [insistance]               [insistance]  \n",
      "3                                      [rappel]                   [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]  \n",
      "...                                         ...                        ...  \n",
      "69892                              [insistance]               [insistance]  \n",
      "69893                              [insistance]               [insistance]  \n",
      "69894                              [insistance]               [insistance]  \n",
      "69895                              [insistance]               [insistance]  \n",
      "69896                              [insistance]               [insistance]  \n",
      "\n",
      "[69885 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('\\033[94m')\n",
    "df_clean1 = []\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df['tokens']:\n",
    "    clean_tokens = []  # Liste pour stocker les tokens nettoyés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Vérifier si le mot n'est pas un mot d'arrêt et n'est pas un signe de ponctuation\n",
    "        if word == 'pas' or word == 'ne' or (word not in stopwords_french and word not in string.punctuation):\n",
    "            clean_tokens.append(word)\n",
    "    # Ajouter les tokens nettoyés de ce texte à la liste df_clean\n",
    "    df_clean1.append(clean_tokens)\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_clean comme une nouvelle colonne à votre DataFrame\n",
    "df['clean_tokens'] = df_clean1\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec03e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                comment         score  \\\n",
      "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
      "1                    doute plus pour efficacite produit   Réclamation   \n",
      "2                          jai reclame regler situation         Refus   \n",
      "3                    jamais recu echantillon auparavant         Refus   \n",
      "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
      "...                                                 ...           ...   \n",
      "2569                                    vient prescrire        Client   \n",
      "2570                      vient prescrire moment visite        Client   \n",
      "2571                                      visite rappel  Présentation   \n",
      "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
      "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [aussi, absorption, directement, niveau, peau,...   \n",
      "1              [doute, plus, pour, efficacite, produit]   \n",
      "2                     [jai, reclame, regler, situation]   \n",
      "3               [jamais, recu, echantillon, auparavant]   \n",
      "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
      "...                                                 ...   \n",
      "2569                                 [vient, prescrire]   \n",
      "2570                 [vient, prescrire, moment, visite]   \n",
      "2571                                   [visite, rappel]   \n",
      "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
      "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
      "\n",
      "                                           clean_tokens  \n",
      "0     [aussi, absorption, directement, niveau, peau,...  \n",
      "1                    [doute, plus, efficacite, produit]  \n",
      "2                     [jai, reclame, regler, situation]  \n",
      "3               [jamais, recu, echantillon, auparavant]  \n",
      "4     [meme, sil, observe, antecedent, cheloide, eme...  \n",
      "...                                                 ...  \n",
      "2569                                 [vient, prescrire]  \n",
      "2570                 [vient, prescrire, moment, visite]  \n",
      "2571                                   [visite, rappel]  \n",
      "2572  [voi, cest, probleme, essentiellement, gastrique]  \n",
      "2573  [voit, pas, beaucoup, mai, ancien, oui, peut, ...  \n",
      "\n",
      "[2574 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_clean2 = []\n",
    "\n",
    "for tokens in df_labled['tokens']:\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if word == 'pas' or word == 'ne' or (word not in stopwords_french and word not in string.punctuation):\n",
    "            clean_tokens.append(word)\n",
    "    df_clean2.append(clean_tokens)\n",
    "\n",
    "df_labled['clean_tokens'] = df_clean2\n",
    "\n",
    "print(df_labled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81ea3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized  \n",
      "0                 bon retour  \n",
      "1                 insistance  \n",
      "2                 insistance  \n",
      "3                     rappel  \n",
      "4       a bon retour produit  \n",
      "...                      ...  \n",
      "69892             insistance  \n",
      "69893             insistance  \n",
      "69894             insistance  \n",
      "69895             insistance  \n",
      "69896             insistance  \n",
      "\n",
      "[69885 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Créer une nouvelle colonne 'clean_data_noTokenized' en rejoignant les tokens nettoyés en une seule chaîne de caractères\n",
    "df['clean_data_noTokenized'] = df['clean_tokens'].apply(' '.join)\n",
    "df_labled['clean_data_noTokenized'] = df_labled['clean_tokens'].apply(' '.join)\n",
    "\n",
    "# Afficher le DataFrame avec la nouvelle colonne 'clean_data_noTokenized'\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fe675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Écrire le DataFrame avec les deux colonnes dans un fichier Excel\n",
    "# df[['comment', 'clean_data_noTokenized']].to_excel('../resources/common/CleanedData/cleaned_comments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc04e7",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f11b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized              lemmatized_tokens  \n",
      "0                 bon retour                  [bon, retour]  \n",
      "1                 insistance                   [insistance]  \n",
      "2                 insistance                   [insistance]  \n",
      "3                     rappel                       [rappel]  \n",
      "4       a bon retour produit  [avoir, bon, retour, produit]  \n",
      "...                      ...                            ...  \n",
      "69892             insistance                   [insistance]  \n",
      "69893             insistance                   [insistance]  \n",
      "69894             insistance                   [insistance]  \n",
      "69895             insistance                   [insistance]  \n",
      "69896             insistance                   [insistance]  \n",
      "\n",
      "[69885 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de langue français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Créer une liste vide pour stocker les tokens lemmatisés\n",
    "df_lemmatized1 = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df_clean1:\n",
    "    lemmatized_tokens = []  # Liste pour stocker les tokens lemmatisés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Lemmatisation du mot\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else word\n",
    "        lemmatized_tokens.append(lemma)  # Ajouter à la liste\n",
    "    # Ajouter les tokens lemmatisés de ce texte à la liste df_lemmatized\n",
    "    df_lemmatized1.append(lemmatized_tokens)\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(df_lemmatized1):\n",
    "    text_str = ' '.join(text)\n",
    "    text_str = re.sub(r'\\bprescr\\w*\\b', 'prescrire', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bsat\\b|\\bst\\b|\\bsatisfaire\\b|\\bsatisfaisant\\b|\\bsatisfaites\\b', 'satisfait', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\brap(el|elle|ell)\\b', 'rappel', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bbc\\b|\\bbcp\\b', 'beaucoup', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\baimer\\b|\\baime\\b', 'aimer', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\btre\\b', 'tres', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bretou\\b', 'retour', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blindication\\b', 'indication', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bpresentationell\\b', 'presentation', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blutilis\\b', 'utiliser', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blachete\\b', 'acheter', text_str, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remettre la chaîne de texte modifiée dans la liste df_lemmatized\n",
    "    df_lemmatized1[i] = text_str.split()\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_lemmatized comme une nouvelle colonne à votre DataFrame\n",
    "df['lemmatized_tokens'] = df_lemmatized1\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e1bd729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized              lemmatized_tokens  \n",
      "0                 bon retour                  [bon, retour]  \n",
      "1                 insistance                   [insistance]  \n",
      "2                 insistance                   [insistance]  \n",
      "3                     rappel                       [rappel]  \n",
      "4       a bon retour produit  [avoir, bon, retour, produit]  \n",
      "...                      ...                            ...  \n",
      "69892             insistance                   [insistance]  \n",
      "69893             insistance                   [insistance]  \n",
      "69894             insistance                   [insistance]  \n",
      "69895             insistance                   [insistance]  \n",
      "69896             insistance                   [insistance]  \n",
      "\n",
      "[69885 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de langue français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Créer une liste vide pour stocker les tokens lemmatisés\n",
    "df_lemmatized2 = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df_clean2:\n",
    "    lemmatized_tokens = []  # Liste pour stocker les tokens lemmatisés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Lemmatisation du mot\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else word\n",
    "        lemmatized_tokens.append(lemma)  # Ajouter à la liste\n",
    "    # Ajouter les tokens lemmatisés de ce texte à la liste df_lemmatized\n",
    "    df_lemmatized2.append(lemmatized_tokens)\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(df_lemmatized2):\n",
    "    # Convertir la liste de tokens en une seule chaîne de texte\n",
    "    text_str = ' '.join(text)\n",
    "    text_str = re.sub(r'\\bprescr\\w*\\b', 'prescrire', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bsat\\b|\\bst\\b|\\bsatisfaire\\b|\\bsatisfaisant\\b|\\bsatisfaites\\b', 'satisfait', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\brap(el|elle|ell)\\b', 'rappel', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bbc\\b|\\bbcp\\b', 'beaucoup', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\baimer\\b|\\baime\\b', 'aimer', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\btre\\b', 'tres', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bretou\\b', 'retour', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blindication\\b', 'indication', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\bpresentationell\\b', 'presentation', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blutilis\\b', 'utiliser', text_str, flags=re.IGNORECASE)\n",
    "    text_str = re.sub(r'\\blachete\\b', 'acheter', text_str, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remettre la chaîne de texte modifiée dans la liste df_lemmatized\n",
    "    df_lemmatized2[i] = text_str.split()\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_lemmatized comme une nouvelle colonne à votre DataFrame\n",
    "df_labled['lemmatized_tokens'] = df_lemmatized2\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b5327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Écrir le DataFrame dans un fichier Excel\n",
    "# df.to_excel('../resources/common/CleanedData/Lematized_Comments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202db13",
   "metadata": {},
   "source": [
    "# Most Frequent 50 Lemmatized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b619b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescrire: 35479\n",
      "rappel: 25293\n",
      "produit: 13883\n",
      "presentation: 6686\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivite: 3853\n",
      "bon: 3471\n",
      "retour: 3410\n",
      "avoir: 2255\n",
      "etendue: 1952\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "satisfait: 913\n",
      "promesse: 784\n",
      "demande: 780\n",
      "representation: 762\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 571\n",
      "association: 516\n",
      "place: 485\n",
      "pas: 483\n",
      "traitement: 463\n",
      "patient: 457\n",
      "cheloide: 440\n",
      "relai: 421\n",
      "mise: 401\n",
      "tres: 388\n",
      "stock: 372\n",
      "surtout: 368\n",
      "echantillon: 367\n",
      "tout: 355\n",
      "cas: 340\n",
      "acte: 320\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "indication: 277\n",
      "poste: 269\n",
      "aussi: 249\n",
      "post: 233\n",
      "utilisation: 228\n",
      "comme: 227\n",
      "laser: 222\n",
      "vaginal: 214\n"
     ]
    }
   ],
   "source": [
    "flat_list = [item for sublist in df_lemmatized1 for item in sublist]\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e99877",
   "metadata": {},
   "source": [
    "# Eliminer les mots dont leur frequence inferieure à 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46042f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the list of lemmatized tokens\n",
    "# flat_list = [item for sublist in df['lemmatized_tokens'] for item in sublist]\n",
    "# # Calculate word frequencies\n",
    "# word_freq = Counter(flat_list)\n",
    "# # Display the least frequent words\n",
    "# least_frequent_words = {word: count for word, count in word_freq.items() if count < 250} # you can choose a threshold that suits your data\n",
    "# print(\"Least frequent words:\", least_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52702c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_to_keep = {word for word, count in word_freq.items() if count >= 250}\n",
    "# # Modify the lists of tokens in the DataFrame directly\n",
    "# df['lemmatized_tokens'] = df['lemmatized_tokens'].apply(lambda tokens: [word for word in tokens if word in words_to_keep])\n",
    "# # If you have another DataFrame (df_labled) that you want to apply the same transformation to:\n",
    "# df_labled['lemmatized_tokens'] = df_labled['lemmatized_tokens'].apply(lambda tokens: [word for word in tokens if word in words_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5f4501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescrire: 35479\n",
      "rappel: 25293\n",
      "produit: 13883\n",
      "presentation: 6686\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivite: 3853\n",
      "bon: 3471\n",
      "retour: 3410\n",
      "avoir: 2255\n",
      "etendue: 1952\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "satisfait: 913\n",
      "promesse: 784\n",
      "demande: 780\n",
      "representation: 762\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 571\n",
      "association: 516\n",
      "place: 485\n",
      "pas: 483\n",
      "traitement: 463\n",
      "patient: 457\n",
      "cheloide: 440\n",
      "relai: 421\n",
      "mise: 401\n",
      "tres: 388\n",
      "stock: 372\n",
      "surtout: 368\n",
      "echantillon: 367\n",
      "tout: 355\n",
      "cas: 340\n",
      "acte: 320\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "indication: 277\n",
      "poste: 269\n",
      "aussi: 249\n",
      "post: 233\n",
      "utilisation: 228\n",
      "comme: 227\n",
      "laser: 222\n",
      "vaginal: 214\n"
     ]
    }
   ],
   "source": [
    "# Maintenant, recalculez les fréquences en utilisant la colonne mise à jour du DataFrame\n",
    "new_flat_list = [item for sublist in df['lemmatized_tokens'] for item in sublist]\n",
    "new_word_freq = Counter(new_flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots après le filtrage\n",
    "for word, count in new_word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bee4625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_data_noTokenized</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacite produit</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[doute, plus, pour, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>doute plus efficacite produit</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>[meme, sil, observer, antecedent, cheloide, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>visite rappel</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voit, pas, beaucoup, mai, pour, ancien, oui, ...</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, peut, ...</td>\n",
       "      <td>voit pas beaucoup mai ancien oui peut prescrire</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, pouvoi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score  \\\n",
       "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
       "1                    doute plus pour efficacite produit   Réclamation   \n",
       "2                          jai reclame regler situation         Refus   \n",
       "3                    jamais recu echantillon auparavant         Refus   \n",
       "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
       "...                                                 ...           ...   \n",
       "2569                                    vient prescrire        Client   \n",
       "2570                      vient prescrire moment visite        Client   \n",
       "2571                                      visite rappel  Présentation   \n",
       "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1              [doute, plus, pour, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
       "\n",
       "                                           clean_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, peut, ...   \n",
       "\n",
       "                                 clean_data_noTokenized  \\\n",
       "0     aussi absorption directement niveau peau dit p...   \n",
       "1                         doute plus efficacite produit   \n",
       "2                          jai reclame regler situation   \n",
       "3                    jamais recu echantillon auparavant   \n",
       "4         meme sil observe antecedent cheloide eme jour   \n",
       "...                                                 ...   \n",
       "2569                                    vient prescrire   \n",
       "2570                      vient prescrire moment visite   \n",
       "2571                                      visite rappel   \n",
       "2572        voi cest probleme essentiellement gastrique   \n",
       "2573    voit pas beaucoup mai ancien oui peut prescrire   \n",
       "\n",
       "                                      lemmatized_tokens  \n",
       "0     [aussi, absorption, directement, niveau, peau,...  \n",
       "1                    [doute, plus, efficacite, produit]  \n",
       "2                     [jai, reclame, regler, situation]  \n",
       "3               [jamais, recu, echantillon, auparavant]  \n",
       "4     [meme, sil, observer, antecedent, cheloide, em...  \n",
       "...                                                 ...  \n",
       "2569                                 [venir, prescrire]  \n",
       "2570                 [venir, prescrire, moment, visite]  \n",
       "2571                                   [visite, rappel]  \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]  \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, pouvoi...  \n",
       "\n",
       "[2574 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06ed53",
   "metadata": {},
   "source": [
    "# Eliminer les 600 les moins mots corrélés avec les targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f66e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized              lemmatized_tokens  \\\n",
      "0                 bon retour                  [bon, retour]   \n",
      "1                 insistance                   [insistance]   \n",
      "2                 insistance                   [insistance]   \n",
      "3                     rappel                       [rappel]   \n",
      "4       a bon retour produit  [avoir, bon, retour, produit]   \n",
      "...                      ...                            ...   \n",
      "69892             insistance                   [insistance]   \n",
      "69893             insistance                   [insistance]   \n",
      "69894             insistance                   [insistance]   \n",
      "69895             insistance                   [insistance]   \n",
      "69896             insistance                   [insistance]   \n",
      "\n",
      "            lemmatized_tokens_finale  \n",
      "0                      [bon, retour]  \n",
      "1                       [insistance]  \n",
      "2                       [insistance]  \n",
      "3                           [rappel]  \n",
      "4      [avoir, bon, retour, produit]  \n",
      "...                              ...  \n",
      "69892                   [insistance]  \n",
      "69893                   [insistance]  \n",
      "69894                   [insistance]  \n",
      "69895                   [insistance]  \n",
      "69896                   [insistance]  \n",
      "\n",
      "[69885 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier mots.csv et créer un ensemble de mots à supprimer\n",
    "words_to_remove = set()\n",
    "with open('../resources/common/data/600_mots_moins_freq.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()  # supprime les espaces en début et en fin de ligne\n",
    "        words_to_remove.add(word)\n",
    "\n",
    "# Pour chaque ligne de df['lemmatized_tokens'], supprimez les mots présents dans mots.csv\n",
    "def filter_tokens(tokens):\n",
    "    return [token for token in tokens if token not in words_to_remove]\n",
    "\n",
    "# Appliquer la fonction de filtrage à chaque ligne de lemmatized_tokens\n",
    "df['lemmatized_tokens_finale'] = df['lemmatized_tokens'].apply(filter_tokens)\n",
    "df_labled['lemmatized_tokens_finale'] = df_labled['lemmatized_tokens'].apply(filter_tokens)\n",
    "\n",
    "# Afficher le DataFrame pour vérifier si les modifications ont été appliquées correctement\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e77ed8",
   "metadata": {},
   "source": [
    "# Modeling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc850209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_data_noTokenized</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_tokens_finale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>a bon retour produit</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  \\\n",
       "0                            Bon retour   \n",
       "1                            Insistance   \n",
       "2                            Insistance   \n",
       "3                                Rappel   \n",
       "4      Elle a un bon retour sur produit   \n",
       "...                                 ...   \n",
       "69892                        Insistance   \n",
       "69893                        Insistance   \n",
       "69894                        Insistance   \n",
       "69895                        Insistance   \n",
       "69896                        Insistance   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "      clean_data_noTokenized              lemmatized_tokens  \\\n",
       "0                 bon retour                  [bon, retour]   \n",
       "1                 insistance                   [insistance]   \n",
       "2                 insistance                   [insistance]   \n",
       "3                     rappel                       [rappel]   \n",
       "4       a bon retour produit  [avoir, bon, retour, produit]   \n",
       "...                      ...                            ...   \n",
       "69892             insistance                   [insistance]   \n",
       "69893             insistance                   [insistance]   \n",
       "69894             insistance                   [insistance]   \n",
       "69895             insistance                   [insistance]   \n",
       "69896             insistance                   [insistance]   \n",
       "\n",
       "            lemmatized_tokens_finale  \n",
       "0                      [bon, retour]  \n",
       "1                       [insistance]  \n",
       "2                       [insistance]  \n",
       "3                           [rappel]  \n",
       "4      [avoir, bon, retour, produit]  \n",
       "...                              ...  \n",
       "69892                   [insistance]  \n",
       "69893                   [insistance]  \n",
       "69894                   [insistance]  \n",
       "69895                   [insistance]  \n",
       "69896                   [insistance]  \n",
       "\n",
       "[69885 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a42bae46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_data_noTokenized</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_tokens_finale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[absorption, directement, niveau, dire, pourqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacite produit</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[doute, plus, pour, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>doute plus efficacite produit</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[reclame, regler, situation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>[meme, sil, observer, antecedent, cheloide, em...</td>\n",
       "      <td>[meme, observer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>visite rappel</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, probleme, essentiellement, gastrique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voit, pas, beaucoup, mai, pour, ancien, oui, ...</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, peut, ...</td>\n",
       "      <td>voit pas beaucoup mai ancien oui peut prescrire</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, pouvoi...</td>\n",
       "      <td>[pas, beaucoup, mai, pouvoir, prescrire]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score  \\\n",
       "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
       "1                    doute plus pour efficacite produit   Réclamation   \n",
       "2                          jai reclame regler situation         Refus   \n",
       "3                    jamais recu echantillon auparavant         Refus   \n",
       "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
       "...                                                 ...           ...   \n",
       "2569                                    vient prescrire        Client   \n",
       "2570                      vient prescrire moment visite        Client   \n",
       "2571                                      visite rappel  Présentation   \n",
       "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1              [doute, plus, pour, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
       "\n",
       "                                           clean_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, peut, ...   \n",
       "\n",
       "                                 clean_data_noTokenized  \\\n",
       "0     aussi absorption directement niveau peau dit p...   \n",
       "1                         doute plus efficacite produit   \n",
       "2                          jai reclame regler situation   \n",
       "3                    jamais recu echantillon auparavant   \n",
       "4         meme sil observe antecedent cheloide eme jour   \n",
       "...                                                 ...   \n",
       "2569                                    vient prescrire   \n",
       "2570                      vient prescrire moment visite   \n",
       "2571                                      visite rappel   \n",
       "2572        voi cest probleme essentiellement gastrique   \n",
       "2573    voit pas beaucoup mai ancien oui peut prescrire   \n",
       "\n",
       "                                      lemmatized_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observer, antecedent, cheloide, em...   \n",
       "...                                                 ...   \n",
       "2569                                 [venir, prescrire]   \n",
       "2570                 [venir, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, pouvoi...   \n",
       "\n",
       "                               lemmatized_tokens_finale  \n",
       "0     [absorption, directement, niveau, dire, pourqu...  \n",
       "1                    [doute, plus, efficacite, produit]  \n",
       "2                          [reclame, regler, situation]  \n",
       "3               [jamais, recu, echantillon, auparavant]  \n",
       "4                                      [meme, observer]  \n",
       "...                                                 ...  \n",
       "2569                                 [venir, prescrire]  \n",
       "2570                 [venir, prescrire, moment, visite]  \n",
       "2571                                   [visite, rappel]  \n",
       "2572        [voi, probleme, essentiellement, gastrique]  \n",
       "2573           [pas, beaucoup, mai, pouvoir, prescrire]  \n",
       "\n",
       "[2574 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c202ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../resources/dev_labo/data/processed/cleaned_data.xlsx', index=False)\n",
    "df_labled.to_excel ('../resources/dev_labo/data/processed/cleaned_labled_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5786f0",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0818f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "81/81 [==============================] - 3s 8ms/step - loss: 1.7222 - accuracy: 0.3652\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 1.1944 - accuracy: 0.6033\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.8444 - accuracy: 0.7187\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.6742 - accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.5707 - accuracy: 0.8120\n",
      "Epoch 6/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.5076 - accuracy: 0.8248\n",
      "Epoch 7/10\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.4666 - accuracy: 0.8415\n",
      "Epoch 8/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4354 - accuracy: 0.8504\n",
      "Epoch 9/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3985 - accuracy: 0.8594\n",
      "Epoch 10/10\n",
      "81/81 [==============================] - 1s 10ms/step - loss: 0.3875 - accuracy: 0.8586\n",
      "2184/2184 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0525 - accuracy: 0.9860\n",
      "Epoch 2/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0390 - accuracy: 0.9890\n",
      "Epoch 3/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0348 - accuracy: 0.9897\n",
      "Epoch 4/10\n",
      "2265/2265 [==============================] - 23s 10ms/step - loss: 0.0307 - accuracy: 0.9912\n",
      "Epoch 5/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0276 - accuracy: 0.9921\n",
      "Epoch 6/10\n",
      "2265/2265 [==============================] - 23s 10ms/step - loss: 0.0253 - accuracy: 0.9928\n",
      "Epoch 7/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0239 - accuracy: 0.9934\n",
      "Epoch 8/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0217 - accuracy: 0.9938\n",
      "Epoch 9/10\n",
      "2265/2265 [==============================] - 23s 10ms/step - loss: 0.0198 - accuracy: 0.9945\n",
      "Epoch 10/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0195 - accuracy: 0.9944\n",
      "2184/2184 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Étape 1: Entraînez un modèle sur les données étiquetées\n",
    "\n",
    "small_texts = df_labled['lemmatized_tokens_finale'].values\n",
    "small_labels = df_labled['score'].values\n",
    "\n",
    "# Encodage des étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(small_labels)\n",
    "small_y = to_categorical(encoded_labels)\n",
    "\n",
    "# Tokenization et padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(small_texts)\n",
    "sequences = tokenizer.texts_to_sequences(small_texts)\n",
    "word_index = tokenizer.word_index\n",
    "small_X = pad_sequences(sequences)\n",
    "\n",
    "# Construction du modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 128, input_length=small_X.shape[1]))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(small_y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(small_X, small_y, epochs=10, batch_size=32)\n",
    "\n",
    "# Étape 2: Utilisez le modèle pour prédire des étiquettes pour les données non étiquetées\n",
    "\n",
    "# Supposons que df soit votre DataFrame contenant les données non étiquetées\n",
    "# et que la colonne 'lemmatized_tokens' contient les textes déjà nettoyés, tokenisés et lemmatisés\n",
    "\n",
    "# Recombiner les tokens en texte\n",
    "unlabeled_texts = [' '.join(tokens) for tokens in df['lemmatized_tokens_finale'].values]\n",
    "unlabeled_sequences = tokenizer.texts_to_sequences(unlabeled_texts)\n",
    "unlabeled_X = pad_sequences(unlabeled_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Prédiction\n",
    "predictions = model.predict(unlabeled_X)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Étape 3: Combinez les données étiquetées originales avec les données auto-étiquetées\n",
    "combined_texts = np.concatenate((small_texts, unlabeled_texts))\n",
    "combined_labels = np.concatenate((small_labels, predicted_labels))\n",
    "\n",
    "# Encodage des étiquettes combinées\n",
    "encoded_combined_labels = label_encoder.transform(combined_labels)\n",
    "combined_y = to_categorical(encoded_combined_labels)\n",
    "\n",
    "# Tokenization et padding des textes combinés\n",
    "combined_sequences = tokenizer.texts_to_sequences(combined_texts)\n",
    "combined_X = pad_sequences(combined_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Étape 4: Ré-entraînez le modèle sur le nouvel ensemble de données combiné\n",
    "model.fit(combined_X, combined_y, epochs=10, batch_size=32,shuffle=True)\n",
    "# Préparer les données de la grande dataset pour la prédiction\n",
    "# Comme précédemment mentionné, 'lemmatized_tokens' contient les textes déjà nettoyés, tokenisés et lemmatisés\n",
    "large_unlabeled_texts = [' '.join(tokens) for tokens in df['lemmatized_tokens_finale'].values]\n",
    "large_unlabeled_sequences = tokenizer.texts_to_sequences(large_unlabeled_texts)\n",
    "large_unlabeled_X = pad_sequences(large_unlabeled_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Utiliser le modèle pour prédire les étiquettes de la grande dataset\n",
    "large_predictions = model.predict(large_unlabeled_X)\n",
    "\n",
    "# Convertir les prédictions en étiquettes lisibles\n",
    "large_predicted_labels = np.argmax(large_predictions, axis=1)\n",
    "large_predicted_labels = label_encoder.inverse_transform(large_predicted_labels)\n",
    "\n",
    "# Ajouter les étiquettes prédites à la grande dataset\n",
    "df = df.copy()\n",
    "df['predicted_score_lstm'] = large_predicted_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1640f47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Partenaire', 'Présentation', 'Client', 'promesse', 'Réclamation',\n",
       "       'test en cours', 'Refus'], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['predicted_score_lstm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cdac2",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning with TfidfVectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7636a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données étiquetées\n",
    "df_labled = df_labled.copy()\n",
    "df_labled['lemmatized_tokens_finale'] = df_labled['lemmatized_tokens_finale'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "small_texts = df_labled['lemmatized_tokens_finale'].values\n",
    "small_labels = df_labled['score'].values\n",
    "\n",
    "# Création d'un modèle - pipeline TF-IDF suivi d'un Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Entraînement du modèle avec les données étiquetées\n",
    "model.fit(small_texts, small_labels)\n",
    "\n",
    "# Prédire sur la grande dataset\n",
    "unlabeled_texts = [' '.join(tokens) for tokens in df['lemmatized_tokens_finale'].values]\n",
    "\n",
    "# Utiliser le modèle pour prédire les étiquettes de la grande dataset\n",
    "large_predicted_labels = model.predict(unlabeled_texts)\n",
    "\n",
    "# Ajouter les étiquettes prédites à la grande dataset\n",
    "df = df.copy()\n",
    "df['predicted_score_TfidfVectorizer'] = large_predicted_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38d840",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning with XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db49f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.96186\n",
      "[1]\teval-mlogloss:1.86685\n",
      "[2]\teval-mlogloss:1.78987\n",
      "[3]\teval-mlogloss:1.71884\n",
      "[4]\teval-mlogloss:1.65842\n",
      "[5]\teval-mlogloss:1.60512\n",
      "[6]\teval-mlogloss:1.55866\n",
      "[7]\teval-mlogloss:1.51507\n",
      "[8]\teval-mlogloss:1.47423\n",
      "[9]\teval-mlogloss:1.43520\n",
      "[10]\teval-mlogloss:1.40300\n",
      "[11]\teval-mlogloss:1.37048\n",
      "[12]\teval-mlogloss:1.34180\n",
      "[13]\teval-mlogloss:1.31821\n",
      "[14]\teval-mlogloss:1.29385\n",
      "[15]\teval-mlogloss:1.27151\n",
      "[16]\teval-mlogloss:1.25067\n",
      "[17]\teval-mlogloss:1.23168\n",
      "[18]\teval-mlogloss:1.21331\n",
      "[19]\teval-mlogloss:1.19622\n",
      "[20]\teval-mlogloss:1.18281\n",
      "[21]\teval-mlogloss:1.16738\n",
      "[22]\teval-mlogloss:1.15289\n",
      "[23]\teval-mlogloss:1.14121\n",
      "[24]\teval-mlogloss:1.13185\n",
      "[25]\teval-mlogloss:1.12110\n",
      "[26]\teval-mlogloss:1.10918\n",
      "[27]\teval-mlogloss:1.09852\n",
      "[28]\teval-mlogloss:1.08945\n",
      "[29]\teval-mlogloss:1.08176\n",
      "[30]\teval-mlogloss:1.07329\n",
      "[31]\teval-mlogloss:1.06719\n",
      "[32]\teval-mlogloss:1.05867\n",
      "[33]\teval-mlogloss:1.05252\n",
      "[34]\teval-mlogloss:1.04713\n",
      "[35]\teval-mlogloss:1.04152\n",
      "[36]\teval-mlogloss:1.03568\n",
      "[37]\teval-mlogloss:1.03039\n",
      "[38]\teval-mlogloss:1.02404\n",
      "[39]\teval-mlogloss:1.02055\n",
      "[40]\teval-mlogloss:1.01606\n",
      "[41]\teval-mlogloss:1.01243\n",
      "[42]\teval-mlogloss:1.00924\n",
      "[43]\teval-mlogloss:1.00401\n",
      "[44]\teval-mlogloss:0.99972\n",
      "[45]\teval-mlogloss:0.99707\n",
      "[46]\teval-mlogloss:0.99450\n",
      "[47]\teval-mlogloss:0.99214\n",
      "[48]\teval-mlogloss:0.98866\n",
      "[49]\teval-mlogloss:0.98699\n",
      "[50]\teval-mlogloss:0.98495\n",
      "[51]\teval-mlogloss:0.98176\n",
      "[52]\teval-mlogloss:0.97917\n",
      "[53]\teval-mlogloss:0.97703\n",
      "[54]\teval-mlogloss:0.97524\n",
      "[55]\teval-mlogloss:0.97350\n",
      "[56]\teval-mlogloss:0.97350\n",
      "[57]\teval-mlogloss:0.97194\n",
      "[58]\teval-mlogloss:0.97043\n",
      "[59]\teval-mlogloss:0.96774\n",
      "[60]\teval-mlogloss:0.96638\n",
      "[61]\teval-mlogloss:0.96493\n",
      "[62]\teval-mlogloss:0.96350\n",
      "[63]\teval-mlogloss:0.96319\n",
      "[64]\teval-mlogloss:0.96199\n",
      "[65]\teval-mlogloss:0.96133\n",
      "[66]\teval-mlogloss:0.96041\n",
      "[67]\teval-mlogloss:0.96005\n",
      "[68]\teval-mlogloss:0.95939\n",
      "[69]\teval-mlogloss:0.95830\n",
      "[70]\teval-mlogloss:0.95807\n",
      "[71]\teval-mlogloss:0.95781\n",
      "[72]\teval-mlogloss:0.95702\n",
      "[73]\teval-mlogloss:0.95628\n",
      "[74]\teval-mlogloss:0.95423\n",
      "[75]\teval-mlogloss:0.95551\n",
      "[76]\teval-mlogloss:0.95488\n",
      "[77]\teval-mlogloss:0.95532\n",
      "[78]\teval-mlogloss:0.95501\n",
      "[79]\teval-mlogloss:0.95531\n",
      "[80]\teval-mlogloss:0.95410\n",
      "[81]\teval-mlogloss:0.95431\n",
      "[82]\teval-mlogloss:0.95373\n",
      "[83]\teval-mlogloss:0.95296\n",
      "[84]\teval-mlogloss:0.95167\n",
      "[85]\teval-mlogloss:0.95054\n",
      "[86]\teval-mlogloss:0.95044\n",
      "[87]\teval-mlogloss:0.95019\n",
      "[88]\teval-mlogloss:0.94998\n",
      "[89]\teval-mlogloss:0.94834\n",
      "[90]\teval-mlogloss:0.94769\n",
      "[91]\teval-mlogloss:0.94755\n",
      "[92]\teval-mlogloss:0.94776\n",
      "[93]\teval-mlogloss:0.94736\n",
      "[94]\teval-mlogloss:0.94800\n",
      "[95]\teval-mlogloss:0.94783\n",
      "[96]\teval-mlogloss:0.94751\n",
      "[97]\teval-mlogloss:0.94751\n",
      "[98]\teval-mlogloss:0.94700\n",
      "[99]\teval-mlogloss:0.94655\n",
      "[100]\teval-mlogloss:0.94656\n",
      "[101]\teval-mlogloss:0.94648\n",
      "[102]\teval-mlogloss:0.94738\n",
      "[103]\teval-mlogloss:0.94853\n",
      "[104]\teval-mlogloss:0.94781\n",
      "[105]\teval-mlogloss:0.94782\n",
      "[106]\teval-mlogloss:0.94775\n",
      "[107]\teval-mlogloss:0.94871\n",
      "[108]\teval-mlogloss:0.94892\n",
      "[109]\teval-mlogloss:0.94938\n",
      "[110]\teval-mlogloss:0.94910\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68       141\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.74      0.70      0.72       104\n",
      "           3       0.68      0.73      0.70       142\n",
      "           4       0.73      0.35      0.48        31\n",
      "           5       0.61      0.47      0.53        47\n",
      "           6       0.50      0.43      0.46        14\n",
      "           7       0.74      0.74      0.74        35\n",
      "\n",
      "    accuracy                           0.67       515\n",
      "   macro avg       0.71      0.65      0.67       515\n",
      "weighted avg       0.68      0.67      0.67       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Créer des embeddings de mots avec Word2Vec\n",
    "sentences = [text.split() for text in df_labled['lemmatized_tokens_finale'].values]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"xgbmodel/word2vec.model\")\n",
    "\n",
    "# Convertir les textes en représentations vectorielles en utilisant les embeddings de mots\n",
    "def text_to_vector(text, model):\n",
    "    words = text.split()\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "    return vector / (len(words) + 1e-5)\n",
    "\n",
    "X = np.array([text_to_vector(text, word2vec_model) for text in df_labled['lemmatized_tokens_finale'].values])\n",
    "small_labels = df_labled['score'].values\n",
    "\n",
    "# Encodez les étiquettes en valeurs numériques\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(small_labels)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner un modèle XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(label_encoder.classes_),\n",
    "    'max_depth': 8,  # Augmentation de la profondeur\n",
    "    'eta': 0.1,  # Augmentation du taux d'apprentissage\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,  # Ajout d'un hyperparamètre\n",
    "    'min_child_weight': 1  # Ajout d'un hyperparamètre\n",
    "}\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "bst = xgb.train(params, dtrain, num_boost_round=200, early_stopping_rounds=10, evals=[(dtest, 'eval')])\n",
    "\n",
    "# Évaluer les performances sur l'ensemble de test\n",
    "y_pred = bst.predict(dtest)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Préparer les données non étiquetées\n",
    "X_unlabeled = np.array([text_to_vector(' '.join(tokens), word2vec_model) for tokens in df['lemmatized_tokens_finale'].values])\n",
    "\n",
    "# Utiliser le modèle pour prédire les étiquettes de la grande dataset\n",
    "d_unlabeled = xgb.DMatrix(X_unlabeled)\n",
    "large_predictions = bst.predict(d_unlabeled)\n",
    "\n",
    "# Convertir les prédictions en étiquettes lisibles\n",
    "large_predicted_labels = label_encoder.inverse_transform(large_predictions.astype(int))\n",
    "\n",
    "# Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "df = df.copy()\n",
    "\n",
    "# Add the predicted labels to the DataFrame\n",
    "df['predicted_score_XGB'] = large_predicted_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f882ba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_data_noTokenized</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_tokens_finale</th>\n",
       "      <th>predicted_score_lstm</th>\n",
       "      <th>predicted_score_TfidfVectorizer</th>\n",
       "      <th>predicted_score_XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>bon retour</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>Partenaire</td>\n",
       "      <td>Partenaire</td>\n",
       "      <td>Partenaire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>rappel</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>a bon retour produit</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "      <td>Partenaire</td>\n",
       "      <td>Partenaire</td>\n",
       "      <td>Partenaire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>insistance</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  \\\n",
       "0                            Bon retour   \n",
       "1                            Insistance   \n",
       "2                            Insistance   \n",
       "3                                Rappel   \n",
       "4      Elle a un bon retour sur produit   \n",
       "...                                 ...   \n",
       "69892                        Insistance   \n",
       "69893                        Insistance   \n",
       "69894                        Insistance   \n",
       "69895                        Insistance   \n",
       "69896                        Insistance   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "      clean_data_noTokenized              lemmatized_tokens  \\\n",
       "0                 bon retour                  [bon, retour]   \n",
       "1                 insistance                   [insistance]   \n",
       "2                 insistance                   [insistance]   \n",
       "3                     rappel                       [rappel]   \n",
       "4       a bon retour produit  [avoir, bon, retour, produit]   \n",
       "...                      ...                            ...   \n",
       "69892             insistance                   [insistance]   \n",
       "69893             insistance                   [insistance]   \n",
       "69894             insistance                   [insistance]   \n",
       "69895             insistance                   [insistance]   \n",
       "69896             insistance                   [insistance]   \n",
       "\n",
       "            lemmatized_tokens_finale predicted_score_lstm  \\\n",
       "0                      [bon, retour]           Partenaire   \n",
       "1                       [insistance]         Présentation   \n",
       "2                       [insistance]         Présentation   \n",
       "3                           [rappel]         Présentation   \n",
       "4      [avoir, bon, retour, produit]           Partenaire   \n",
       "...                              ...                  ...   \n",
       "69892                   [insistance]         Présentation   \n",
       "69893                   [insistance]         Présentation   \n",
       "69894                   [insistance]         Présentation   \n",
       "69895                   [insistance]         Présentation   \n",
       "69896                   [insistance]         Présentation   \n",
       "\n",
       "      predicted_score_TfidfVectorizer predicted_score_XGB  \n",
       "0                          Partenaire          Partenaire  \n",
       "1                        Présentation        Présentation  \n",
       "2                        Présentation        Présentation  \n",
       "3                        Présentation        Présentation  \n",
       "4                          Partenaire          Partenaire  \n",
       "...                               ...                 ...  \n",
       "69892                    Présentation        Présentation  \n",
       "69893                    Présentation        Présentation  \n",
       "69894                    Présentation        Présentation  \n",
       "69895                    Présentation        Présentation  \n",
       "69896                    Présentation        Présentation  \n",
       "\n",
       "[69885 rows x 9 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "365dfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les colonnes spécifiques du DataFrame\n",
    "extracted_df = df[['comment', 'predicted_score_lstm']]\n",
    "# Exporter le DataFrame extrait vers un fichier Excel\n",
    "extracted_df.to_excel('../resources/dev_labo/data/processed/Comments_Classification', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78848a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
