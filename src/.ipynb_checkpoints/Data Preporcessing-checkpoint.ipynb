{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac235dfa",
   "metadata": {},
   "source": [
    "\n",
    "# Importing Libraries, Requirements and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "393c7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: autocorrect in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 2)) (3.5.4)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: keras==2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 4)) (2.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 7)) (3.7)\n",
      "Requirement already satisfied: stopwords in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 9)) (1.7.1)\n",
      "Requirement already satisfied: h2o in c:\\users\\msi\\anaconda3\\lib\\site-packages (from -r ../requirements.txt (line 10)) (3.40.0.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (63.4.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (8.1.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.10.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from spacy->-r ../requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.9.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow->-r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.30.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (15.0.6.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.51.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from nltk->-r ../requirements.txt (line 7)) (2021.11.10)\n",
      "Requirement already satisfied: click in c:\\users\\msi\\anaconda3\\lib\\site-packages (from nltk->-r ../requirements.txt (line 7)) (8.0.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\msi\\anaconda3\\lib\\site-packages (from h2o->-r ../requirements.txt (line 10)) (0.8.10)\n",
      "Requirement already satisfied: future in c:\\users\\msi\\anaconda3\\lib\\site-packages (from h2o->-r ../requirements.txt (line 10)) (0.18.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy->-r ../requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ../requirements.txt (line 2)) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 2)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r ../requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->-r ../requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from jinja2->spacy->-r ../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.16.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r ../requirements.txt (line 6)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#Installer tous les packages nécaissaires\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "#ou bien\n",
    "\n",
    "# !pip install autocorrect\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !pip install pyspellchecker\n",
    "# !pip install keras==2.11.0\n",
    "# !pip install xgboost\n",
    "# !pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4715e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "##from keras.models import Sequential\n",
    "#from keras.layers import Embedding, LSTM, Dense\n",
    "#from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa5c7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemins des fichiers XLSX\n",
    "file1 = '../resources/common/data/all_raw_comments_cleaning.xlsx'\n",
    "file4 = '../resources/common/data/labled_comments.xlsx'\n",
    "\n",
    "# Lecture des fichiers XLSX\n",
    "dfnew_comment = pd.read_excel(file1)\n",
    "df_labled = pd.read_excel(file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b963061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit ...</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacité produit</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai réclamé régler situation</td>\n",
       "      <td>Refus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais reçu échantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>même sil observe antécédent chéloïde ème jour</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest problème essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score\n",
       "0      aussi absorption directement niveau peau dit ...   Réclamation\n",
       "1                    doute plus pour efficacité produit   Réclamation\n",
       "2                          jai réclamé régler situation         Refus\n",
       "3                    jamais reçu échantillon auparavant         Refus\n",
       "4         même sil observe antécédent chéloïde ème jour   Réclamation\n",
       "...                                                 ...           ...\n",
       "2569                                    vient prescrire        Client\n",
       "2570                      vient prescrire moment visite        Client\n",
       "2571                                      visite rappel  Présentation\n",
       "2572        voi cest problème essentiellement gastrique   Réclamation\n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation\n",
       "\n",
       "[2574 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c56959d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69897 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment\n",
       "0                            Bon retour\n",
       "1                            Insistance\n",
       "2                            Insistance\n",
       "3                                Rappel\n",
       "4      Elle a un bon retour sur produit\n",
       "...                                 ...\n",
       "69892                        Insistance\n",
       "69893                        Insistance\n",
       "69894                        Insistance\n",
       "69895                        Insistance\n",
       "69896                        Insistance\n",
       "\n",
       "[69897 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew_comment.drop('Unnamed: 1', axis=1, inplace=True)\n",
    "dfnew_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dace52",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa1a6711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Vérifiez si le texte est une chaîne\n",
    "    if isinstance(text, str):\n",
    "        # Supprimer le texte de retweet de style ancien \"RT\"\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "        # Supprimer les hyperliens\n",
    "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "        # Supprimer les hashtags (seulement supprimer le signe de hash # du mot)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        # Supprimer les dates au format AAAA-MM-JJ\n",
    "        text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '', text)\n",
    "        # Supprimer l'heure au format HH:MM ou HH:MM:SS\n",
    "        text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text)\n",
    "        # Supprimer les caractères spéciaux\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Supprimer les lignes vides ou les lignes avec juste un point\n",
    "        text = re.sub(r'^(\\s*\\.?\\s*)$', '', text, flags=re.MULTILINE)\n",
    "        # Supprimer les accents\n",
    "        text = unidecode(text)\n",
    "    else:\n",
    "        # Si le texte est un nombre, convertissez-le en chaîne de caractères\n",
    "        if isinstance(text, (int, float)):\n",
    "            text = str(text)\n",
    "        # Si le texte est une valeur NaN, remplacez-le par une chaîne vide\n",
    "        elif pd.isnull(text):\n",
    "            text = ''\n",
    "    return text.strip()  # Supprimer les espaces blancs en tête ou en queue\n",
    "# Appliquer la fonction à chaque élément du DataFrame\n",
    "dfnew_comment = dfnew_comment.applymap(clean_text)\n",
    "# Remplacer les lignes qui sont juste un point ou une virgule (maintenant une chaîne vide après avoir supprimé les caractères spéciaux) par NaN\n",
    "dfnew_comment.replace(\"\", np.nan, inplace=True)\n",
    "# Supprimer les lignes avec des valeurs NaN\n",
    "dfnew_comment.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_labled['comment'] = df_labled['comment'].apply(clean_text)\n",
    "\n",
    "# Remplacer les lignes qui sont juste un point ou une virgule (maintenant une chaîne vide après avoir supprimé les caractères spéciaux) par NaN\n",
    "df_labled.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Supprimer les lignes avec des valeurs NaN dans la colonne 'comment'\n",
    "df_labled.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "\n",
    "print(dfnew_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "521dffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment\n",
      "0                            Bon retour\n",
      "1                            Insistance\n",
      "2                            Insistance\n",
      "3                                Rappel\n",
      "4      Elle a un bon retour sur produit\n",
      "...                                 ...\n",
      "69892                        Insistance\n",
      "69893                        Insistance\n",
      "69894                        Insistance\n",
      "69895                        Insistance\n",
      "69896                        Insistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Renommer dfnew_comment par df\n",
    "df=dfnew_comment\n",
    "print(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9839ab",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6af1c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     comment\n",
      "0                            \u001b[92mBon retour\n",
      "1                            \u001b[92mInsistance\n",
      "2                            \u001b[92mInsistance\n",
      "3                                \u001b[92mRappel\n",
      "4      \u001b[92mElle a un bon retour sur produit\n",
      "...                                      ...\n",
      "69892                        \u001b[92mInsistance\n",
      "69893                        \u001b[92mInsistance\n",
      "69894                        \u001b[92mInsistance\n",
      "69895                        \u001b[92mInsistance\n",
      "69896                        \u001b[92mInsistance\n",
      "\n",
      "[69885 rows x 1 columns]\n",
      "\u001b[94m\n",
      "\n",
      "DataFrame après tokenisation:\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print('\\033[92m' + df)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# Tokeniser les textes dans la colonne 'comment'\n",
    "df['tokens'] = df['comment'].apply(tokenizer.tokenize)\n",
    "df_labled['tokens'] = df_labled['comment'].apply(tokenizer.tokenize)\n",
    "# Afficher le DataFrame après tokenisation\n",
    "print(\"\\nDataFrame après tokenisation:\")\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7d64946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens  \n",
      "0                                 [bon, retour]  \n",
      "1                                  [insistance]  \n",
      "2                                  [insistance]  \n",
      "3                                      [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  \n",
      "...                                         ...  \n",
      "69892                              [insistance]  \n",
      "69893                              [insistance]  \n",
      "69894                              [insistance]  \n",
      "69895                              [insistance]  \n",
      "69896                              [insistance]  \n",
      "\n",
      "[69885 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6bb85",
   "metadata": {},
   "source": [
    "# Importing French Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8404624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords_french = stopwords.words('french')\n",
    "print('Stop words\\n')\n",
    "print(stopwords_french)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7e7c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \n",
      "0                                 [bon, retour]              [bon, retour]  \n",
      "1                                  [insistance]               [insistance]  \n",
      "2                                  [insistance]               [insistance]  \n",
      "3                                      [rappel]                   [rappel]  \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]  \n",
      "...                                         ...                        ...  \n",
      "69892                              [insistance]               [insistance]  \n",
      "69893                              [insistance]               [insistance]  \n",
      "69894                              [insistance]               [insistance]  \n",
      "69895                              [insistance]               [insistance]  \n",
      "69896                              [insistance]               [insistance]  \n",
      "\n",
      "[69885 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('\\033[94m')\n",
    "df_clean1 = []\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df['tokens']:\n",
    "    clean_tokens = []  # Liste pour stocker les tokens nettoyés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Vérifier si le mot n'est pas un mot d'arrêt et n'est pas un signe de ponctuation\n",
    "        if word not in stopwords_french and word not in string.punctuation:\n",
    "            clean_tokens.append(word)\n",
    "    # Ajouter les tokens nettoyés de ce texte à la liste df_clean\n",
    "    df_clean1.append(clean_tokens)\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_clean comme une nouvelle colonne à votre DataFrame\n",
    "df['clean_tokens'] = df_clean1\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ec03e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                comment         score  \\\n",
      "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
      "1                    doute plus pour efficacite produit   Réclamation   \n",
      "2                          jai reclame regler situation         Refus   \n",
      "3                    jamais recu echantillon auparavant         Refus   \n",
      "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
      "...                                                 ...           ...   \n",
      "2569                                    vient prescrire        Client   \n",
      "2570                      vient prescrire moment visite        Client   \n",
      "2571                                      visite rappel  Présentation   \n",
      "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
      "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [aussi, absorption, directement, niveau, peau,...   \n",
      "1              [doute, plus, pour, efficacite, produit]   \n",
      "2                     [jai, reclame, regler, situation]   \n",
      "3               [jamais, recu, echantillon, auparavant]   \n",
      "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
      "...                                                 ...   \n",
      "2569                                 [vient, prescrire]   \n",
      "2570                 [vient, prescrire, moment, visite]   \n",
      "2571                                   [visite, rappel]   \n",
      "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
      "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
      "\n",
      "                                           clean_tokens  \n",
      "0     [aussi, absorption, directement, niveau, peau,...  \n",
      "1                    [doute, plus, efficacite, produit]  \n",
      "2                     [jai, reclame, regler, situation]  \n",
      "3               [jamais, recu, echantillon, auparavant]  \n",
      "4     [meme, sil, observe, antecedent, cheloide, eme...  \n",
      "...                                                 ...  \n",
      "2569                                 [vient, prescrire]  \n",
      "2570                 [vient, prescrire, moment, visite]  \n",
      "2571                                   [visite, rappel]  \n",
      "2572  [voi, cest, probleme, essentiellement, gastrique]  \n",
      "2573  [voit, beaucoup, mai, ancien, oui, peut, presc...  \n",
      "\n",
      "[2574 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_clean2 = []\n",
    "\n",
    "for tokens in df_labled['tokens']:\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords_french and word not in string.punctuation:\n",
    "            clean_tokens.append(word)\n",
    "    df_clean2.append(clean_tokens)\n",
    "\n",
    "df_labled['clean_tokens'] = df_clean2\n",
    "\n",
    "print(df_labled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d81ea3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized  \n",
      "0                 bon retour  \n",
      "1                 insistance  \n",
      "2                 insistance  \n",
      "3                     rappel  \n",
      "4       a bon retour produit  \n",
      "...                      ...  \n",
      "69892             insistance  \n",
      "69893             insistance  \n",
      "69894             insistance  \n",
      "69895             insistance  \n",
      "69896             insistance  \n",
      "\n",
      "[69885 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Créer une nouvelle colonne 'clean_data_noTokenized' en rejoignant les tokens nettoyés en une seule chaîne de caractères\n",
    "df['clean_data_noTokenized'] = df['clean_tokens'].apply(' '.join)\n",
    "df_labled['clean_data_noTokenized'] = df_labled['clean_tokens'].apply(' '.join)\n",
    "\n",
    "# Afficher le DataFrame avec la nouvelle colonne 'clean_data_noTokenized'\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89fe675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire le DataFrame avec les deux colonnes dans un fichier Excel\n",
    "df[['comment', 'clean_data_noTokenized']].to_excel('../resources/common/CleanedData/cleaned_comments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc04e7",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f11b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized              lemmatized_tokens  \n",
      "0                 bon retour                  [bon, retour]  \n",
      "1                 insistance                   [insistance]  \n",
      "2                 insistance                   [insistance]  \n",
      "3                     rappel                       [rappel]  \n",
      "4       a bon retour produit  [avoir, bon, retour, produit]  \n",
      "...                      ...                            ...  \n",
      "69892             insistance                   [insistance]  \n",
      "69893             insistance                   [insistance]  \n",
      "69894             insistance                   [insistance]  \n",
      "69895             insistance                   [insistance]  \n",
      "69896             insistance                   [insistance]  \n",
      "\n",
      "[69885 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de langue français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Créer une liste vide pour stocker les tokens lemmatisés\n",
    "df_lemmatized1 = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df_clean1:\n",
    "    lemmatized_tokens = []  # Liste pour stocker les tokens lemmatisés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Lemmatisation du mot\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else word\n",
    "        lemmatized_tokens.append(lemma)  # Ajouter à la liste\n",
    "    # Ajouter les tokens lemmatisés de ce texte à la liste df_lemmatized\n",
    "    df_lemmatized1.append(lemmatized_tokens)\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(df_lemmatized1):\n",
    "    # Convertir la liste de tokens en une seule chaîne de texte\n",
    "    text_str = ' '.join(text)\n",
    "    \n",
    "    # 1) Remplacer les mots par 'prescrire'\n",
    "    text_str = re.sub(r'\\bprescr\\w*\\b', 'prescrire', text_str)\n",
    "    \n",
    "    # 2) Remplacer les mots par 'satisfait'\n",
    "    text_str = re.sub(r'\\b(?:sat\\w*|st\\w*|satisfaire)\\b', 'satisfait', text_str)\n",
    "    \n",
    "    # Remettre la chaîne de texte modifiée dans la liste df_lemmatized\n",
    "    df_lemmatized1[i] = text_str.split()\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_lemmatized comme une nouvelle colonne à votre DataFrame\n",
    "df['lemmatized_tokens'] = df_lemmatized1\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e1bd729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                comment  \\\n",
      "0                            Bon retour   \n",
      "1                            Insistance   \n",
      "2                            Insistance   \n",
      "3                                Rappel   \n",
      "4      Elle a un bon retour sur produit   \n",
      "...                                 ...   \n",
      "69892                        Insistance   \n",
      "69893                        Insistance   \n",
      "69894                        Insistance   \n",
      "69895                        Insistance   \n",
      "69896                        Insistance   \n",
      "\n",
      "                                         tokens               clean_tokens  \\\n",
      "0                                 [bon, retour]              [bon, retour]   \n",
      "1                                  [insistance]               [insistance]   \n",
      "2                                  [insistance]               [insistance]   \n",
      "3                                      [rappel]                   [rappel]   \n",
      "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
      "...                                         ...                        ...   \n",
      "69892                              [insistance]               [insistance]   \n",
      "69893                              [insistance]               [insistance]   \n",
      "69894                              [insistance]               [insistance]   \n",
      "69895                              [insistance]               [insistance]   \n",
      "69896                              [insistance]               [insistance]   \n",
      "\n",
      "      clean_data_noTokenized              lemmatized_tokens  \n",
      "0                 bon retour                  [bon, retour]  \n",
      "1                 insistance                   [insistance]  \n",
      "2                 insistance                   [insistance]  \n",
      "3                     rappel                       [rappel]  \n",
      "4       a bon retour produit  [avoir, bon, retour, produit]  \n",
      "...                      ...                            ...  \n",
      "69892             insistance                   [insistance]  \n",
      "69893             insistance                   [insistance]  \n",
      "69894             insistance                   [insistance]  \n",
      "69895             insistance                   [insistance]  \n",
      "69896             insistance                   [insistance]  \n",
      "\n",
      "[69885 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de langue français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Créer une liste vide pour stocker les tokens lemmatisés\n",
    "df_lemmatized2 = []\n",
    "\n",
    "# Parcourir chaque liste de tokens\n",
    "for tokens in df_clean2:\n",
    "    lemmatized_tokens = []  # Liste pour stocker les tokens lemmatisés d'un texte particulier\n",
    "    for word in tokens:  # Parcourir chaque mot dans la liste de tokens\n",
    "        # Lemmatisation du mot\n",
    "        doc = nlp(word)\n",
    "        lemma = doc[0].lemma_ if doc else word\n",
    "        lemmatized_tokens.append(lemma)  # Ajouter à la liste\n",
    "    # Ajouter les tokens lemmatisés de ce texte à la liste df_lemmatized\n",
    "    df_lemmatized2.append(lemmatized_tokens)\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(df_lemmatized2):\n",
    "    # Convertir la liste de tokens en une seule chaîne de texte\n",
    "    text_str = ' '.join(text)\n",
    "    \n",
    "    # 1) Remplacer les mots par 'prescrire'\n",
    "    text_str = re.sub(r'\\bprescr\\w*\\b', 'prescrire', text_str)\n",
    "    \n",
    "    # 2) Remplacer les mots par 'satisfait'\n",
    "    text_str = re.sub(r'\\b(?:sat\\w*|st\\w*|satisfaire)\\b', 'satisfait', text_str)\n",
    "    \n",
    "    # Remettre la chaîne de texte modifiée dans la liste df_lemmatized\n",
    "    df_lemmatized2[i] = text_str.split()\n",
    "\n",
    "# Vous pouvez maintenant ajouter df_lemmatized comme une nouvelle colonne à votre DataFrame\n",
    "df_labled['lemmatized_tokens'] = df_lemmatized2\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5b5327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrir le DataFrame dans un fichier Excel\n",
    "df.to_excel('../resources/common/CleanedData/Lematized_Comments.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202db13",
   "metadata": {},
   "source": [
    "# Most Frequent 50 Lemmatized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b619b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescrire: 35479\n",
      "rappel: 25278\n",
      "produit: 13883\n",
      "presentation: 6662\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivite: 3853\n",
      "bon: 3471\n",
      "retour: 3409\n",
      "avoir: 2255\n",
      "etendue: 1952\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "satisfait: 1322\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "promesse: 784\n",
      "demande: 780\n",
      "representation: 762\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 571\n",
      "association: 516\n",
      "place: 485\n",
      "traitement: 463\n",
      "patient: 457\n",
      "cheloide: 440\n",
      "relai: 421\n",
      "mise: 401\n",
      "tre: 388\n",
      "surtout: 368\n",
      "echantillon: 367\n",
      "tout: 355\n",
      "cas: 340\n",
      "acte: 320\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "poste: 269\n",
      "aussi: 249\n",
      "post: 233\n",
      "utilisation: 228\n",
      "comme: 227\n",
      "laser: 222\n",
      "vaginal: 214\n",
      "condidose: 208\n",
      "apre: 200\n",
      "efficace: 190\n"
     ]
    }
   ],
   "source": [
    "flat_list = [item for sublist in df_lemmatized1 for item in sublist]\n",
    "# Compter les occurrences de chaque mot\n",
    "word_freq = Counter(flat_list)\n",
    "# Afficher les fréquences de mots\n",
    "for word, count in word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9139d17",
   "metadata": {},
   "source": [
    "# Eliminer les mots dont leur frequence inferieure à 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46042f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in df['lemmatized_tokens'] for item in sublist]\n",
    "word_freq = Counter(flat_list)\n",
    "\n",
    "# Créer un ensemble de mots à conserver (ceux qui ont une fréquence d'au moins 200)\n",
    "# Notez que j'utilise une valeur plus petite pour l'exemple, vous devriez utiliser 200.\n",
    "words_to_keep = {word for word, count in word_freq.items() if count >= 250}\n",
    "\n",
    "# Modifier directement les listes de tokens dans le DataFrame\n",
    "df['lemmatized_tokens'] = df['lemmatized_tokens'].apply(lambda tokens: [word for word in tokens if word in words_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5f4501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescrire: 35479\n",
      "rappel: 25278\n",
      "produit: 13883\n",
      "presentation: 6662\n",
      "insistance: 4762\n",
      "aller: 4586\n",
      "entrain: 4337\n",
      "exclusivite: 3853\n",
      "bon: 3471\n",
      "retour: 3409\n",
      "avoir: 2255\n",
      "etendue: 1952\n",
      "surface: 1591\n",
      "conseil: 1347\n",
      "satisfait: 1322\n",
      "fois: 1008\n",
      "quelque: 997\n",
      "promesse: 784\n",
      "demande: 780\n",
      "representation: 762\n",
      "disponible: 626\n",
      "plaie: 617\n",
      "gramme: 584\n",
      "mentionner: 571\n",
      "association: 516\n",
      "place: 485\n",
      "traitement: 463\n",
      "patient: 457\n",
      "cheloide: 440\n",
      "relai: 421\n",
      "mise: 401\n",
      "tre: 388\n",
      "surtout: 368\n",
      "echantillon: 367\n",
      "tout: 355\n",
      "cas: 340\n",
      "acte: 320\n",
      "exclusivement: 301\n",
      "engagement: 291\n",
      "bien: 290\n",
      "poste: 269\n"
     ]
    }
   ],
   "source": [
    "# Vous avez déjà mis à jour la colonne 'lemmatized_tokens' de votre DataFrame\n",
    "# en utilisant le code que vous avez montré précédemment.\n",
    "\n",
    "# Maintenant, recalculez les fréquences en utilisant la colonne mise à jour du DataFrame\n",
    "new_flat_list = [item for sublist in df['lemmatized_tokens'] for item in sublist]\n",
    "new_word_freq = Counter(new_flat_list)\n",
    "\n",
    "# Afficher les fréquences de mots après le filtrage\n",
    "for word, count in new_word_freq.most_common(50):  # Affiche les 50 mots les plus fréquents\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c6c7d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_data_noTokenized</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacite produit</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[doute, plus, pour, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>doute plus efficacite produit</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>[meme, sil, observer, antecedent, cheloide, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>visite rappel</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voit, pas, beaucoup, mai, pour, ancien, oui, ...</td>\n",
       "      <td>[voit, beaucoup, mai, ancien, oui, peut, presc...</td>\n",
       "      <td>voit beaucoup mai ancien oui peut prescrire</td>\n",
       "      <td>[voit, beaucoup, mai, ancien, oui, pouvoir, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score  \\\n",
       "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
       "1                    doute plus pour efficacite produit   Réclamation   \n",
       "2                          jai reclame regler situation         Refus   \n",
       "3                    jamais recu echantillon auparavant         Refus   \n",
       "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
       "...                                                 ...           ...   \n",
       "2569                                    vient prescrire        Client   \n",
       "2570                      vient prescrire moment visite        Client   \n",
       "2571                                      visite rappel  Présentation   \n",
       "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1              [doute, plus, pour, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
       "\n",
       "                                           clean_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, beaucoup, mai, ancien, oui, peut, presc...   \n",
       "\n",
       "                                 clean_data_noTokenized  \\\n",
       "0     aussi absorption directement niveau peau dit p...   \n",
       "1                         doute plus efficacite produit   \n",
       "2                          jai reclame regler situation   \n",
       "3                    jamais recu echantillon auparavant   \n",
       "4         meme sil observe antecedent cheloide eme jour   \n",
       "...                                                 ...   \n",
       "2569                                    vient prescrire   \n",
       "2570                      vient prescrire moment visite   \n",
       "2571                                      visite rappel   \n",
       "2572        voi cest probleme essentiellement gastrique   \n",
       "2573        voit beaucoup mai ancien oui peut prescrire   \n",
       "\n",
       "                                      lemmatized_tokens  \n",
       "0     [aussi, absorption, directement, niveau, peau,...  \n",
       "1                    [doute, plus, efficacite, produit]  \n",
       "2                     [jai, reclame, regler, situation]  \n",
       "3               [jamais, recu, echantillon, auparavant]  \n",
       "4     [meme, sil, observer, antecedent, cheloide, em...  \n",
       "...                                                 ...  \n",
       "2569                                 [venir, prescrire]  \n",
       "2570                 [venir, prescrire, moment, visite]  \n",
       "2571                                   [visite, rappel]  \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]  \n",
       "2573  [voit, beaucoup, mai, ancien, oui, pouvoir, pr...  \n",
       "\n",
       "[2574 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95ecadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrir le DataFrame dans un fichier Excel\n",
    "df_labled.to_excel('../resources/common/CleanedData/Lematized_labled_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb85e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
