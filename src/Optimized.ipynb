{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac235dfa",
   "metadata": {},
   "source": [
    "\n",
    "# Importing Libraries, Requirements and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128228dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installer tous les packages nécaissaires\n",
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4715e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bedhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_french = stopwords.words('french')\n",
    "print('Stop words\\n')\n",
    "print(stopwords_french)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5c7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemins des fichiers XLSX\n",
    "file1 = '../resources/common/data/labled_comments.xlsx'\n",
    "file2 = '../resources/common/data/all_raw_comments_cleaning.xlsx'\n",
    "# Lecture des fichiers XLSX\n",
    "dflabled = pd.read_excel(file1)\n",
    "dfnew_comment= pd.read_excel(file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd483bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assurez-vous d'avoir le modèle français de spaCy installé\n",
    "# Vous pouvez l'installer en utilisant : python -m spacy download fr_core_news_sm\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Liste des stopwords français\n",
    "stopwords_french = set(stopwords.words('french'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '', text)\n",
    "        text = re.sub(r'\\b\\d{2}:\\d{2}(:\\d{2})?\\b', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'^(\\s*\\.?\\s*)$', '', text, flags=re.MULTILINE)\n",
    "        text = unidecode(text)\n",
    "    else:\n",
    "        if isinstance(text, (int, float)):\n",
    "            text = str(text)\n",
    "        elif pd.isnull(text):\n",
    "            text = ''\n",
    "    return text.strip()\n",
    "\n",
    "def clean_and_process_text(df, column_name):\n",
    "    # Appliquer clean_text à la colonne spécifiée\n",
    "    df[column_name] = df[column_name].apply(clean_text)\n",
    "    \n",
    "    # Remplacer les chaînes vides par NaN et supprimer les lignes avec NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(subset=[column_name], inplace=True)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    df['tokens'] = df[column_name].apply(tokenizer.tokenize)\n",
    "    \n",
    "    # Suppression des stopwords et ponctuation\n",
    "    df_clean = []\n",
    "    for tokens in df['tokens']:\n",
    "        clean_tokens = [word for word in tokens if word == 'pas' or word == 'ne' or (word not in stopwords_french and word not in string.punctuation)]\n",
    "        df_clean.append(clean_tokens)\n",
    "    \n",
    "    # Ajouter les tokens nettoyés comme une nouvelle colonne\n",
    "    df['clean_tokens'] = df_clean\n",
    "    \n",
    "    # Lemmatisation et autres remplacements\n",
    "    df_lemmatized = []\n",
    "    for tokens in df_clean:\n",
    "        lemmatized_tokens = []\n",
    "        for word in tokens:\n",
    "            doc = nlp(word)\n",
    "            lemma = doc[0].lemma_ if doc else word\n",
    "            lemmatized_tokens.append(lemma)\n",
    "        \n",
    "        # (Ici vous pouvez ajouter d'autres remplacements comme vous avez montré dans votre code# (suite du code)\n",
    "        text_str = ' '.join(lemmatized_tokens)\n",
    "        \n",
    "        # Appliquer divers remplacements\n",
    "        text_str = re.sub(r'\\bprescr\\w*\\b', 'prescrire', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\bsat\\b|\\bst\\b|\\bsatisfaire\\b|\\bsatisfaisant\\b|\\bsatisfaites\\b', 'satisfait', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\brap(el|elle)\\b', 'rappel', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\bbc\\b|\\bbcp\\b', 'beaucoup', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\baimer\\b|\\baime\\b', 'aimer', text_str, flags=re.IGNORECASE)\n",
    "        #text_str = re.sub(r'\\baucun\\b|\\baucune\\b|\\bne\\b|\\bpas\\b|\\babsence\\b|\\babsant\\b|\\bnon\\b', 'negative', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\btre\\b', 'tres', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\bretou\\b', 'retour', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\blindication\\b', 'indication', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\bpresentationell\\b', 'presentation', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\blutilis\\b', 'utiliser', text_str, flags=re.IGNORECASE)\n",
    "        text_str = re.sub(r'\\blachete\\b', 'acheter', text_str, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Ajouter à la liste\n",
    "        lemmatized_tokens = text_str.split()\n",
    "        df_lemmatized.append(lemmatized_tokens)\n",
    "    \n",
    "    # Ajouter les tokens lemmatisés comme une nouvelle colonne\n",
    "    df['lemmatized_tokens'] = df_lemmatized\n",
    "   \n",
    "\n",
    "    # Lire le fichier mots.csv et créer un ensemble de mots à supprimer\n",
    "    words_to_remove = set()\n",
    "    with open('../resources/common/data/600_mots_moins_freq.csv', 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()  # supprime les espaces en début et en fin de ligne\n",
    "            words_to_remove.add(word)\n",
    "    \n",
    "    # Pour chaque ligne de df['lemmatized_tokens'], supprimez les mots présents dans mots.csv\n",
    "    def filter_tokens(tokens):\n",
    "        return [token for token in tokens if token not in words_to_remove]\n",
    "    \n",
    "    # Appliquer la fonction de filtrage à chaque ligne de lemmatized_tokens\n",
    "    df['lemmatized_tokens_filtred'] = df['lemmatized_tokens'].apply(filter_tokens)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e6fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_tokens_filtred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aussi absorption directement niveau peau dit p...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[aussi, absorption, directement, niveau, peau,...</td>\n",
       "      <td>[absorption, directement, niveau, dire, pourqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doute plus pour efficacite produit</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[doute, plus, pour, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "      <td>[doute, plus, efficacite, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jai reclame regler situation</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[jai, reclame, regler, situation]</td>\n",
       "      <td>[reclame, regler, situation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamais recu echantillon auparavant</td>\n",
       "      <td>Refus</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "      <td>[jamais, recu, echantillon, auparavant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meme sil observe antecedent cheloide eme jour</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>[meme, sil, observe, antecedent, cheloide, eme...</td>\n",
       "      <td>[meme, sil, observer, antecedent, cheloide, em...</td>\n",
       "      <td>[meme, observer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>vient prescrire</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>[vient, prescrire]</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "      <td>[venir, prescrire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>vient prescrire moment visite</td>\n",
       "      <td>Client</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>[vient, prescrire, moment, visite]</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "      <td>[venir, prescrire, moment, visite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>visite rappel</td>\n",
       "      <td>Présentation</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "      <td>[visite, rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>voi cest probleme essentiellement gastrique</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, cest, probleme, essentiellement, gastrique]</td>\n",
       "      <td>[voi, probleme, essentiellement, gastrique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>voit pas beaucoup  mai pour ancien oui peut pr...</td>\n",
       "      <td>Réclamation</td>\n",
       "      <td>[voit, pas, beaucoup, mai, pour, ancien, oui, ...</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, peut, ...</td>\n",
       "      <td>[voit, pas, beaucoup, mai, ancien, oui, pouvoi...</td>\n",
       "      <td>[pas, beaucoup, mai, pouvoir, prescrire]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2574 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment         score  \\\n",
       "0     aussi absorption directement niveau peau dit p...   Réclamation   \n",
       "1                    doute plus pour efficacite produit   Réclamation   \n",
       "2                          jai reclame regler situation         Refus   \n",
       "3                    jamais recu echantillon auparavant         Refus   \n",
       "4         meme sil observe antecedent cheloide eme jour   Réclamation   \n",
       "...                                                 ...           ...   \n",
       "2569                                    vient prescrire        Client   \n",
       "2570                      vient prescrire moment visite        Client   \n",
       "2571                                      visite rappel  Présentation   \n",
       "2572        voi cest probleme essentiellement gastrique   Réclamation   \n",
       "2573  voit pas beaucoup  mai pour ancien oui peut pr...   Réclamation   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1              [doute, plus, pour, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, pour, ancien, oui, ...   \n",
       "\n",
       "                                           clean_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observe, antecedent, cheloide, eme...   \n",
       "...                                                 ...   \n",
       "2569                                 [vient, prescrire]   \n",
       "2570                 [vient, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, peut, ...   \n",
       "\n",
       "                                      lemmatized_tokens  \\\n",
       "0     [aussi, absorption, directement, niveau, peau,...   \n",
       "1                    [doute, plus, efficacite, produit]   \n",
       "2                     [jai, reclame, regler, situation]   \n",
       "3               [jamais, recu, echantillon, auparavant]   \n",
       "4     [meme, sil, observer, antecedent, cheloide, em...   \n",
       "...                                                 ...   \n",
       "2569                                 [venir, prescrire]   \n",
       "2570                 [venir, prescrire, moment, visite]   \n",
       "2571                                   [visite, rappel]   \n",
       "2572  [voi, cest, probleme, essentiellement, gastrique]   \n",
       "2573  [voit, pas, beaucoup, mai, ancien, oui, pouvoi...   \n",
       "\n",
       "                              lemmatized_tokens_filtred  \n",
       "0     [absorption, directement, niveau, dire, pourqu...  \n",
       "1                    [doute, plus, efficacite, produit]  \n",
       "2                          [reclame, regler, situation]  \n",
       "3               [jamais, recu, echantillon, auparavant]  \n",
       "4                                      [meme, observer]  \n",
       "...                                                 ...  \n",
       "2569                                 [venir, prescrire]  \n",
       "2570                 [venir, prescrire, moment, visite]  \n",
       "2571                                   [visite, rappel]  \n",
       "2572        [voi, probleme, essentiellement, gastrique]  \n",
       "2573           [pas, beaucoup, mai, pouvoir, prescrire]  \n",
       "\n",
       "[2574 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_labled = clean_and_process_text(dflabled, 'comment')\n",
    "df_processed_labled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a686f2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_tokens_filtred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "      <td>[bon, retour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rappel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "      <td>[rappel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle a un bon retour sur produit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[elle, a, un, bon, retour, sur, produit]</td>\n",
       "      <td>[a, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "      <td>[avoir, bon, retour, produit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69893</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69894</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69895</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69896</th>\n",
       "      <td>Insistance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "      <td>[insistance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69885 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                comment  Unnamed: 1  \\\n",
       "0                            Bon retour         NaN   \n",
       "1                            Insistance         NaN   \n",
       "2                            Insistance         NaN   \n",
       "3                                Rappel         NaN   \n",
       "4      Elle a un bon retour sur produit         NaN   \n",
       "...                                 ...         ...   \n",
       "69892                        Insistance         NaN   \n",
       "69893                        Insistance         NaN   \n",
       "69894                        Insistance         NaN   \n",
       "69895                        Insistance         NaN   \n",
       "69896                        Insistance         NaN   \n",
       "\n",
       "                                         tokens               clean_tokens  \\\n",
       "0                                 [bon, retour]              [bon, retour]   \n",
       "1                                  [insistance]               [insistance]   \n",
       "2                                  [insistance]               [insistance]   \n",
       "3                                      [rappel]                   [rappel]   \n",
       "4      [elle, a, un, bon, retour, sur, produit]  [a, bon, retour, produit]   \n",
       "...                                         ...                        ...   \n",
       "69892                              [insistance]               [insistance]   \n",
       "69893                              [insistance]               [insistance]   \n",
       "69894                              [insistance]               [insistance]   \n",
       "69895                              [insistance]               [insistance]   \n",
       "69896                              [insistance]               [insistance]   \n",
       "\n",
       "                   lemmatized_tokens      lemmatized_tokens_filtred  \n",
       "0                      [bon, retour]                  [bon, retour]  \n",
       "1                       [insistance]                   [insistance]  \n",
       "2                       [insistance]                   [insistance]  \n",
       "3                           [rappel]                       [rappel]  \n",
       "4      [avoir, bon, retour, produit]  [avoir, bon, retour, produit]  \n",
       "...                              ...                            ...  \n",
       "69892                   [insistance]                   [insistance]  \n",
       "69893                   [insistance]                   [insistance]  \n",
       "69894                   [insistance]                   [insistance]  \n",
       "69895                   [insistance]                   [insistance]  \n",
       "69896                   [insistance]                   [insistance]  \n",
       "\n",
       "[69885 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_new = clean_and_process_text(dfnew_comment, 'comment')\n",
    "df_processed_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95ad7d63",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1590f5f4",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf1cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "81/81 [==============================] - 2s 10ms/step - loss: 1.7299 - accuracy: 0.3489\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 1.1751 - accuracy: 0.6002\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.8247 - accuracy: 0.7222\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.6483 - accuracy: 0.7937\n",
      "Epoch 5/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.5816 - accuracy: 0.8069\n",
      "Epoch 6/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.5212 - accuracy: 0.8306\n",
      "Epoch 7/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4675 - accuracy: 0.8407\n",
      "Epoch 8/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.4265 - accuracy: 0.8473\n",
      "Epoch 9/10\n",
      "81/81 [==============================] - 1s 10ms/step - loss: 0.4074 - accuracy: 0.8613\n",
      "Epoch 10/10\n",
      "81/81 [==============================] - 1s 9ms/step - loss: 0.3745 - accuracy: 0.8722\n",
      "2184/2184 [==============================] - 5s 2ms/step\n",
      "Epoch 1/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0508 - accuracy: 0.9855\n",
      "Epoch 2/10\n",
      "2265/2265 [==============================] - 20s 9ms/step - loss: 0.0394 - accuracy: 0.9883\n",
      "Epoch 3/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0344 - accuracy: 0.9899\n",
      "Epoch 4/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0308 - accuracy: 0.9911\n",
      "Epoch 5/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0284 - accuracy: 0.9915\n",
      "Epoch 6/10\n",
      "2265/2265 [==============================] - 21s 9ms/step - loss: 0.0250 - accuracy: 0.9930\n",
      "Epoch 7/10\n",
      "2265/2265 [==============================] - 20s 9ms/step - loss: 0.0235 - accuracy: 0.9933\n",
      "Epoch 8/10\n",
      "2265/2265 [==============================] - 20s 9ms/step - loss: 0.0216 - accuracy: 0.9940\n",
      "Epoch 9/10\n",
      "2265/2265 [==============================] - 22s 10ms/step - loss: 0.0204 - accuracy: 0.9940\n",
      "Epoch 10/10\n",
      "2265/2265 [==============================] - 23s 10ms/step - loss: 0.0188 - accuracy: 0.9948\n",
      "2184/2184 [==============================] - 4s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          Partenaire\n",
       "1        Présentation\n",
       "2        Présentation\n",
       "3        Présentation\n",
       "4          Partenaire\n",
       "             ...     \n",
       "69892    Présentation\n",
       "69893    Présentation\n",
       "69894    Présentation\n",
       "69895    Présentation\n",
       "69896    Présentation\n",
       "Name: predicted_score_lstm, Length: 69885, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Étape 1: Entraînez un modèle sur les données étiquetées\n",
    "\n",
    "small_texts = df_processed_labled['lemmatized_tokens_filtred'].values\n",
    "small_labels = dflabled['score'].values\n",
    "\n",
    "# Encodage des étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(small_labels)\n",
    "small_y = to_categorical(encoded_labels)\n",
    "\n",
    "# Tokenization et padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(small_texts)\n",
    "sequences = tokenizer.texts_to_sequences(small_texts)\n",
    "word_index = tokenizer.word_index\n",
    "small_X = pad_sequences(sequences)\n",
    "\n",
    "# Construction du modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 128, input_length=small_X.shape[1]))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(small_y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(small_X, small_y, epochs=10, batch_size=32)\n",
    "\n",
    "# Étape 2: Utilisez le modèle pour prédire des étiquettes pour les données non étiquetées\n",
    "\n",
    "# Supposons que df soit votre DataFrame contenant les données non étiquetées\n",
    "# et que la colonne 'lemmatized_tokens' contient les textes déjà nettoyés, tokenisés et lemmatisés\n",
    "\n",
    "# Recombiner les tokens en texte\n",
    "unlabeled_texts = [' '.join(tokens) for tokens in df_processed_new['lemmatized_tokens_filtred'].values]\n",
    "unlabeled_sequences = tokenizer.texts_to_sequences(unlabeled_texts)\n",
    "unlabeled_X = pad_sequences(unlabeled_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Prédiction\n",
    "predictions = model.predict(unlabeled_X)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Étape 3: Combinez les données étiquetées originales avec les données auto-étiquetées\n",
    "combined_texts = np.concatenate((small_texts, unlabeled_texts))\n",
    "combined_labels = np.concatenate((small_labels, predicted_labels))\n",
    "\n",
    "# Encodage des étiquettes combinées\n",
    "encoded_combined_labels = label_encoder.transform(combined_labels)\n",
    "combined_y = to_categorical(encoded_combined_labels)\n",
    "\n",
    "# Tokenization et padding des textes combinés\n",
    "combined_sequences = tokenizer.texts_to_sequences(combined_texts)\n",
    "combined_X = pad_sequences(combined_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Étape 4: Ré-entraînez le modèle sur le nouvel ensemble de données combiné\n",
    "model.fit(combined_X, combined_y, epochs=10, batch_size=32,shuffle=True)\n",
    "# Préparer les données de la grande dataset pour la prédiction\n",
    "# Comme précédemment mentionné, 'lemmatized_tokens' contient les textes déjà nettoyés, tokenisés et lemmatisés\n",
    "large_unlabeled_texts = [' '.join(tokens) for tokens in df_processed_new['lemmatized_tokens_filtred'].values]\n",
    "large_unlabeled_sequences = tokenizer.texts_to_sequences(large_unlabeled_texts)\n",
    "large_unlabeled_X = pad_sequences(large_unlabeled_sequences, maxlen=small_X.shape[1])\n",
    "\n",
    "# Utiliser le modèle pour prédire les étiquettes de la grande dataset\n",
    "large_predictions = model.predict(large_unlabeled_X)\n",
    "\n",
    "# Convertir les prédictions en étiquettes lisibles\n",
    "large_predicted_labels = np.argmax(large_predictions, axis=1)\n",
    "large_predicted_labels = label_encoder.inverse_transform(large_predicted_labels)\n",
    "\n",
    "# Ajouter les étiquettes prédites à la grande dataset\n",
    "df_processed_new['predicted_score_lstm'] = large_predicted_labels\n",
    "df_processed_new['predicted_score_lstm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621a7cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Partenaire', 'Présentation', 'Client', 'promesse', 'Réclamation',\n",
       "       'test en cours', 'Refus', 'Livraison'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_new['predicted_score_lstm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e106eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_new.to_excel('../resources/common/resultat.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8451bef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Prédiction pour la nouvelle donnée: ['Présentation']\n"
     ]
    }
   ],
   "source": [
    "# Étape 1: Prétraiter la nouvelle donnée\n",
    "nouvelle_data = \"aucune remarque\"   # à remplacer par votre texte\n",
    "\n",
    "# Créer un DataFrame avec une seule colonne 'comment'\n",
    "df = pd.DataFrame({'comment': [nouvelle_data]})\n",
    "\n",
    "# Vous devriez effectuer le même prétraitement que pour vos données initiales (par exemple, lemmatisation, etc.)\n",
    "clean_and_process_text(df, 'comment')\n",
    "\n",
    "# Vous devriez maintenant extraire la nouvelle donnée prétraitée du DataFrame\n",
    "nouvelle_data_preprocessed = df.iloc[0]['comment']\n",
    "\n",
    "# Étape 2: Tokenization et padding\n",
    "nouvelle_sequence = tokenizer.texts_to_sequences([nouvelle_data_preprocessed])\n",
    "nouvelle_X = pad_sequences(nouvelle_sequence, maxlen=small_X.shape[1])\n",
    "\n",
    "# Étape 3: Utilisez le modèle pour faire une prédiction\n",
    "prediction = model.predict(nouvelle_X)\n",
    "\n",
    "# Étape 4: Décoder la prédiction pour obtenir l'étiquette correspondante\n",
    "predicted_label = np.argmax(prediction, axis=1)\n",
    "predicted_label = label_encoder.inverse_transform(predicted_label)\n",
    "\n",
    "print(f\"Prédiction pour la nouvelle donnée: {predicted_label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010471f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
